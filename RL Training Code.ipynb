{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxWT4qFbNHPc",
        "outputId": "dfe8a8ae-96cc-4816-ff47-a5c3f5da6986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting trl==0.11.3\n",
            "  Downloading trl-0.11.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.3) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.3) (4.55.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from trl==0.11.3) (1.10.0)\n",
            "Collecting tyro>=0.5.11 (from trl==0.11.3)\n",
            "  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from trl==0.11.3) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->trl==0.11.3) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.3) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->trl==0.11.3) (0.6.2)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.3) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.3) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.3)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.11.3) (4.4.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->trl==0.11.3) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->trl==0.11.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->trl==0.11.3) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (0.1.2)\n",
            "Downloading trl-0.11.3-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: shtab, tyro, trl\n",
            "Successfully installed shtab-1.7.2 trl-0.11.3 tyro-0.9.28\n"
          ]
        }
      ],
      "source": [
        "!pip install trl==0.11.3 -U datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ3l2GV7NFbO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from datasets import Dataset as HFDataset\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import wandb\n",
        "import time\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "C-NNAwpHNG12",
        "outputId": "e53ec0c5-4e9d-4354-f105-324147c0a0eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericzhangez1006\u001b[0m (\u001b[33mericzhangez1006-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "434WcYt9dLCj",
        "outputId": "1d2be829-d914-478c-ea1f-ffe16d76ba1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting PPO training with Weights & Biases integration...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ppo/objective/entropy</td><td>▁▁▁▁▁▂▁▁▁▁▁▁▂▂▂▂▂███▇▇▆▆▄▄▄▃▃▃▃▂▃▂▃▂▃▂▃▂</td></tr><tr><td>ppo/objective/kl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/objective/kl_coef</td><td>█████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>ppo/ppo/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/loss/policy</td><td>▇▃▃▃▃▁▂▁▃▂▂▂▂▃▂▁▁▁▁▂▄▂▃▃▃▄▅▅▆▅█▆▆▆▆▅▅▄▆▆</td></tr><tr><td>ppo/ppo/loss/total</td><td>█▇▅▄▄▃▃▃▃▂▃▅▃▂▂▂▃▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁▂▁▁▁▂▁</td></tr><tr><td>ppo/ppo/loss/value</td><td>▂▇█▆▄▃▃▃▃▅▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▂▁▂▁▂▂▁▂▁▁▂▁▂</td></tr><tr><td>ppo/ppo/mean_non_score_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/mean_scores</td><td>▅▅▃▄▅▆▇█▇▆▆▆▇▆▇▅▇▂▂▁▁▁▂▁▂▂▁▂▂▁▂▃▂▂▂▂▃▂▃▃</td></tr><tr><td>ppo/ppo/policy/advantages_mean</td><td>▁▃▄▁▅▅▄▅▅▅▄▅▃▅▄▄▄▅▅▅▄█▅▅▅▅▅▄▄▄▄▄▄▅▄▄▄▄▄▄</td></tr><tr><td>ppo/ppo/policy/approxkl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁█▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/policy/clipfrac</td><td>▆▃▄▃▃▄▅▆▃▄▅▅▄▅▅▄▆▅▆▅▇▇▇█▆▃▃▃▃▃▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/policy/entropy</td><td>▅▅▅▅▅▅▅▄▄▄▄▄▅▄▅▄▆▅▅▄██▆▅▃▅▃▂▂▁▂▂▁▁▂▂▁▂▁▁</td></tr><tr><td>ppo/ppo/policy/policykl</td><td>▂▂▂▂▂▃▂▂▂▂▂▂▃▁▃▂▃▁▂▂█▅▂▂▂▂▂▃▂▂▃▂▁▂▂▂▂▂▂▂</td></tr><tr><td>ppo/ppo/returns/mean</td><td>▅▅▆▅▄▇█▇▇▆▇▆▆▇▆▅▅▅█▅▆▃▃▃▂▂▁▁▁▁▂▂▂▂▂▂▂▃▂▃</td></tr><tr><td>ppo/ppo/returns/var</td><td>▅▄▄▄▄▄▃▄▄▄▄▃▆▄▅▅▃▂▂▃▄▂▁▁▃▂▂▂▃▃▃▃▃▃▃▅▃▅█▇</td></tr><tr><td>ppo/ppo/std_scores</td><td>█▇██▇▇▅█▅▁▇▅▆▅▅▇▇▇█▇▆▇▅▄▆▂▅▆▃▄▂▅▃▇▄▁▄▅▄▄</td></tr><tr><td>ppo/ppo/val/clipfrac</td><td>▅▇▆▇▃▂▃▁▇▆█▁▅▃▃▁▄▃▂▂▂▆▅▄▂▂▂▂▂▁▁▁▃▄▃▂▃▂▃▂</td></tr><tr><td>ppo/ppo/val/error</td><td>▇█▄▃▄▂▃▃▂▃▂▂▂▃▂▃▂▂▃▂▂▂▂▂▁▂▁▂▂▂▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>ppo/ppo/val/mean</td><td>▇██▇▆▅▅▅▅▅▇▇▇▇▆▆▆▆▆▆▄▃▃▂▂▁▁▁▁▁▂▂▂▂▁▂▂▂▁▃</td></tr><tr><td>ppo/ppo/val/var</td><td>▁▁▁▁▁▂▁▁▁▁▁▁▃▁▅▂▅▅▄▄▅▄▃▃▃▃▂▂▁▁▂▁▂▂▃▄▄▅▅█</td></tr><tr><td>ppo/ppo/val/var_explained</td><td>▁▄▅▄▄▆▄▅▅▇▆▆▅▇▇▆▇▇▇▇▇▇▇▇▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>ppo/ppo/val/vpred</td><td>██▇▆▅▅▆▇▇▇▇▇▇▇▇▇▅▆▅▆▇▆▆▆▄▃▃▃▁▁▁▁▁▂▂▂▂▂▂▃</td></tr><tr><td>ppo/time/ppo/calc_stats</td><td>▁▁▄▇▂▃█▅▃▁▁▂▂▁▂█▂▂██████████████████████</td></tr><tr><td>ppo/time/ppo/compute_advantages</td><td>▁▃█▂▄▃▅▃█▇▁█▁▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>ppo/time/ppo/compute_rewards</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/time/ppo/forward_pass</td><td>▁▆▄▅█▇▁▂▂▁▄▂▄▁▇▂██▆█▆▇▇▇▅▆▇▇▆▆▆▆▆▄▇▇▅▆▄▆</td></tr><tr><td>ppo/time/ppo/optimize_step</td><td>▂▄▇▄▃▃▂▂▁▁▁▃▁▁█████████████▇█▇▇▇███▇▇██▇</td></tr><tr><td>ppo/time/ppo/total</td><td>▇▂▃▃▂▃▄▅▅▂▂▁▂▁▁▁▂█████████████▇██▇▇▇██▇█</td></tr><tr><td>ppo/tokens/queries_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/tokens/queries_len_std</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/tokens/responses_len_mean</td><td>▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▂▃▁▄▆▆▅▇▇▇▇██████▇█████</td></tr><tr><td>ppo/tokens/responses_len_std</td><td>▂▃▃▃▃▃▃▅▂▂▂▂▂▂▅▂▅▅▇▅█▆▅█▅▆▇▇▆▆▁▆▁▅▁▅▅▇▅▁</td></tr><tr><td>train/batch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/batch_emotion</td><td>█▆▆▇█▇▇▇█▇█▇▇▇██▇▇█▇▇█▆█▇▇▅▂▂▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train/batch_emotion_success_rate</td><td>▇▆▆█▇███▇█▇▇▇██▇█▇▇█▇▇█▇▇█▇█▆▅▆▄▂▁▂▁▂▂▂▄</td></tr><tr><td>train/batch_empathy</td><td>▅▅▁▆▃▄▅▅▃▆▄▁▅▅▅▅▆▆▆▅▄▇▆▇▇█▇███▆█▇▆▇▇▇█▇▇</td></tr><tr><td>train/batch_quality</td><td>▅▄▆▅▆▇▆▇▇▇▆▇██▇██▆▆▇▂▂▃▄▁▂▄▅▅▃▃▃▂▄▅▃▄▂▆▆</td></tr><tr><td>train/batch_relevance</td><td>▇▇▆▃██▆▆▆▆▄▅▃▇▆▄▇█▄▅▄▆▅▆▅▄▃▃▂▅▁▁▂▃▄▂▅▄▃▅</td></tr><tr><td>train/batch_reward</td><td>▅▄▅▄▆▆▄▅▆▇▇▇▇▇▆▆▆▆█▅▂▃▁▁▁▁▂▃▂▂▁▂▂▃▂▂▂▁▂▃</td></tr><tr><td>train/batch_sentiment</td><td>▆█▄▅▄▇▆▇▄▅▃▆▅▄▅▆▄▇▆▃▅▂▄▄▃▁▃▂▁▂▂▃█▄▄▇▃▃▇▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ppo/objective/entropy</td><td>108.64667</td></tr><tr><td>ppo/objective/kl</td><td>0</td></tr><tr><td>ppo/objective/kl_coef</td><td>0.1932</td></tr><tr><td>ppo/ppo/learning_rate</td><td>2e-05</td></tr><tr><td>ppo/ppo/loss/policy</td><td>-0.00957</td></tr><tr><td>ppo/ppo/loss/total</td><td>0.20326</td></tr><tr><td>ppo/ppo/loss/value</td><td>2.12825</td></tr><tr><td>ppo/ppo/mean_non_score_reward</td><td>0</td></tr><tr><td>ppo/ppo/mean_scores</td><td>-3.32949</td></tr><tr><td>ppo/ppo/policy/advantages_mean</td><td>0.0</td></tr><tr><td>ppo/ppo/policy/approxkl</td><td>0.02785</td></tr><tr><td>ppo/ppo/policy/clipfrac</td><td>0.03076</td></tr><tr><td>ppo/ppo/policy/entropy</td><td>0.2582</td></tr><tr><td>ppo/ppo/policy/policykl</td><td>-0.01815</td></tr><tr><td>ppo/ppo/returns/mean</td><td>-4.41842</td></tr><tr><td>ppo/ppo/returns/var</td><td>38.93427</td></tr><tr><td>ppo/ppo/std_scores</td><td>5.74171</td></tr><tr><td>ppo/ppo/val/clipfrac</td><td>0.23584</td></tr><tr><td>ppo/ppo/val/error</td><td>4.08168</td></tr><tr><td>ppo/ppo/val/mean</td><td>-4.46842</td></tr><tr><td>ppo/ppo/val/var</td><td>29.42367</td></tr><tr><td>ppo/ppo/val/var_explained</td><td>0.89516</td></tr><tr><td>ppo/ppo/val/vpred</td><td>-4.39353</td></tr><tr><td>ppo/time/ppo/calc_stats</td><td>0.13341</td></tr><tr><td>ppo/time/ppo/compute_advantages</td><td>0.02118</td></tr><tr><td>ppo/time/ppo/compute_rewards</td><td>0.00309</td></tr><tr><td>ppo/time/ppo/forward_pass</td><td>0.18677</td></tr><tr><td>ppo/time/ppo/optimize_step</td><td>0.83852</td></tr><tr><td>ppo/time/ppo/total</td><td>1.18313</td></tr><tr><td>ppo/tokens/queries_len_mean</td><td>128</td></tr><tr><td>ppo/tokens/queries_len_std</td><td>0</td></tr><tr><td>ppo/tokens/responses_len_mean</td><td>128</td></tr><tr><td>ppo/tokens/responses_len_std</td><td>0</td></tr><tr><td>train/batch</td><td>108</td></tr><tr><td>train/batch_emotion</td><td>-6.3125</td></tr><tr><td>train/batch_emotion_success_rate</td><td>0.6875</td></tr><tr><td>train/batch_empathy</td><td>0.89284</td></tr><tr><td>train/batch_quality</td><td>0.21875</td></tr><tr><td>train/batch_relevance</td><td>-1.375</td></tr><tr><td>train/batch_reward</td><td>-3.32949</td></tr><tr><td>train/batch_sentiment</td><td>0.1847</td></tr><tr><td>train/epoch</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">8m19therapy_ppo_enhanced_rewards</strong> at: <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/wrpa38cp' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/wrpa38cp</a><br> View project at: <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced</a><br>Synced 5 W&B file(s), 11 media file(s), 22 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250819_173653-wrpa38cp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250819_182837-agsy9y6e</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e' target=\"_blank\">8m19therapy_ppo_enhanced_rewards</a></strong> to <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B run: 8m19therapy_ppo_enhanced_rewards\n",
            "W&B url: https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e\n",
            "Loading tokenizer...\n",
            "Tokenizer vocabulary size: 50269\n",
            "Loading base model...\n",
            "Loading SFT checkpoint from /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/therapy_model_4thFIXED_epoch_7_loss_2.2373.ckpt...\n",
            "SFT checkpoint loaded successfully\n",
            "Loading similarity model...\n",
            "Similarity model loaded successfully\n",
            "Loading sentiment and empathy models...\n",
            "Sentiment and empathy models loaded successfully\n",
            "Creating PPO configuration...\n",
            "Creating PPO models...\n",
            "Loading datasets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded tokenizer from /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/train_processed_4thFIXED_tokenizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing data: 100%|██████████| 815/815 [00:02<00:00, 335.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 3721 therapist responses\n",
            "Loaded tokenizer from /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/train_processed_4thFIXED_tokenizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing data: 100%|██████████| 102/102 [00:00<00:00, 317.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 445 therapist responses\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 3721\n",
            "Validation dataset size: 445\n",
            "Initializing Custom PPO trainer...\n",
            "Starting PPO training...\n",
            "\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:04<17:33,  4.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=6.349, Quality=2.000, Emotion=3.000, Relevance=0.562, Sentiment=0.348, Empathy=0.853, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [00:30<19:54,  5.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=1.391, Quality=1.250, Emotion=2.438, Relevance=-2.812, Sentiment=0.394, Empathy=0.756, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [00:57<20:23,  5.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=2.429, Quality=-1.000, Emotion=2.188, Relevance=-0.688, Sentiment=0.357, Empathy=0.704, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [01:24<19:16,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=3.454, Quality=-0.250, Emotion=2.375, Relevance=0.750, Sentiment=0.345, Empathy=0.832, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [01:51<18:21,  5.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=4.879, Quality=-0.250, Emotion=3.000, Relevance=1.375, Sentiment=0.377, Empathy=0.852, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [02:15<17:32,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=2.210, Quality=-1.000, Emotion=1.750, Relevance=1.125, Sentiment=0.229, Empathy=0.841, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [02:41<16:55,  5.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=3.498, Quality=0.500, Emotion=1.312, Relevance=1.375, Sentiment=0.432, Empathy=0.859, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [03:08<17:24,  5.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=3.673, Quality=-0.250, Emotion=1.875, Relevance=1.438, Sentiment=0.550, Empathy=0.847, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [03:34<16:21,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=3.049, Quality=0.500, Emotion=2.375, Relevance=-0.375, Sentiment=0.169, Empathy=0.832, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [03:58<15:41,  5.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=3.684, Quality=-1.000, Emotion=2.812, Relevance=1.125, Sentiment=0.379, Empathy=0.853, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [04:24<15:29,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=4.778, Quality=0.500, Emotion=2.250, Relevance=1.188, Sentiment=0.441, Empathy=0.825, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [04:48<14:08,  4.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=3.532, Quality=0.500, Emotion=2.375, Relevance=-0.500, Sentiment=0.588, Empathy=0.862, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [05:13<14:50,  5.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=2.409, Quality=0.500, Emotion=0.562, Relevance=0.750, Sentiment=0.580, Empathy=0.870, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [05:37<13:38,  4.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=2.598, Quality=-0.500, Emotion=1.750, Relevance=0.500, Sentiment=0.199, Empathy=0.838, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [06:01<13:13,  4.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=-0.009, Quality=-0.250, Emotion=1.188, Relevance=-1.500, Sentiment=0.249, Empathy=0.861, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [06:26<12:43,  4.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=1.683, Quality=-0.250, Emotion=1.750, Relevance=-0.062, Sentiment=0.263, Empathy=0.841, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [06:49<11:59,  4.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=3.734, Quality=0.500, Emotion=0.750, Relevance=0.938, Sentiment=0.122, Empathy=0.795, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [07:16<12:55,  5.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=1.326, Quality=-1.000, Emotion=1.750, Relevance=0.062, Sentiment=0.463, Empathy=0.798, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [07:38<10:55,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=1.348, Quality=2.000, Emotion=0.750, Relevance=-2.375, Sentiment=0.393, Empathy=0.849, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [08:03<11:32,  5.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=3.984, Quality=-1.000, Emotion=2.375, Relevance=0.875, Sentiment=0.357, Empathy=0.757, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [08:28<10:57,  4.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=2.407, Quality=-1.000, Emotion=2.188, Relevance=-0.250, Sentiment=0.097, Empathy=0.810, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [08:51<10:12,  4.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=3.099, Quality=-1.000, Emotion=3.000, Relevance=0.500, Sentiment=0.315, Empathy=0.839, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [09:15<10:33,  5.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=1.135, Quality=-0.250, Emotion=1.125, Relevance=-1.625, Sentiment=0.395, Empathy=0.849, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [09:39<08:52,  4.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=1.594, Quality=1.500, Emotion=1.750, Relevance=-2.875, Sentiment=0.224, Empathy=0.848, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [10:03<08:49,  4.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=1.358, Quality=-0.250, Emotion=0.625, Relevance=0.625, Sentiment=0.379, Empathy=0.833, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [10:28<08:36,  4.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=4.212, Quality=1.250, Emotion=2.188, Relevance=0.250, Sentiment=0.388, Empathy=0.863, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [10:53<08:18,  4.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=4.280, Quality=2.000, Emotion=1.125, Relevance=0.500, Sentiment=0.210, Empathy=0.836, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [11:15<07:05,  4.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=1.638, Quality=0.500, Emotion=0.500, Relevance=-0.312, Sentiment=0.288, Empathy=0.827, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [11:36<06:53,  4.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=3.032, Quality=1.250, Emotion=0.500, Relevance=0.688, Sentiment=0.071, Empathy=0.871, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [11:59<06:40,  4.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=4.411, Quality=0.500, Emotion=1.750, Relevance=0.562, Sentiment=0.191, Empathy=0.820, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [12:23<06:34,  4.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=6.027, Quality=1.250, Emotion=2.438, Relevance=1.625, Sentiment=0.277, Empathy=0.858, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [12:49<06:34,  5.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=2.865, Quality=0.500, Emotion=1.812, Relevance=0.000, Sentiment=0.352, Empathy=0.851, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [13:16<06:16,  5.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=2.200, Quality=0.500, Emotion=1.750, Relevance=-0.562, Sentiment=0.289, Empathy=0.867, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [13:38<05:08,  4.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=2.275, Quality=1.250, Emotion=0.938, Relevance=-0.750, Sentiment=0.297, Empathy=0.833, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [14:00<04:28,  4.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=-0.000, Quality=0.500, Emotion=0.500, Relevance=-2.250, Sentiment=0.187, Empathy=0.844, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [14:23<04:23,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=2.571, Quality=0.500, Emotion=1.750, Relevance=-0.188, Sentiment=0.131, Empathy=0.848, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [14:46<04:05,  4.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=1.501, Quality=0.500, Emotion=1.125, Relevance=-0.875, Sentiment=0.216, Empathy=0.844, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [15:09<03:39,  4.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=4.697, Quality=0.500, Emotion=2.375, Relevance=1.000, Sentiment=0.413, Empathy=0.800, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [15:32<03:21,  4.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=3.858, Quality=1.250, Emotion=2.375, Relevance=-0.688, Sentiment=0.422, Empathy=0.878, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [15:57<02:57,  4.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=2.534, Quality=-0.250, Emotion=1.750, Relevance=0.625, Sentiment=0.147, Empathy=0.846, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [16:22<02:42,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=1.385, Quality=-0.250, Emotion=0.500, Relevance=-0.312, Sentiment=0.196, Empathy=0.866, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [16:46<02:06,  4.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=4.140, Quality=2.000, Emotion=1.125, Relevance=0.438, Sentiment=0.219, Empathy=0.857, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [17:09<01:44,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=2.245, Quality=-0.250, Emotion=1.750, Relevance=-1.312, Sentiment=0.539, Empathy=0.809, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [17:36<01:29,  5.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=5.296, Quality=1.250, Emotion=3.000, Relevance=0.312, Sentiment=0.409, Empathy=0.853, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [18:01<01:02,  5.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=2.159, Quality=0.500, Emotion=1.750, Relevance=-0.438, Sentiment=0.236, Empathy=0.870, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [18:27<00:35,  5.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=3.891, Quality=2.000, Emotion=0.500, Relevance=0.750, Sentiment=0.334, Empathy=0.851, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [18:54<00:10,  5.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=5.720, Quality=0.500, Emotion=3.000, Relevance=1.188, Sentiment=0.422, Empathy=0.845, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [19:01<00:00,  4.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 1 Results:\n",
            "Average total reward: 2.8942\n",
            "Average quality score: 0.2940\n",
            "Average emotion score: 1.8492\n",
            "Average relevance score: -0.0992\n",
            "Average sentiment score: 0.3238\n",
            "Average empathy score: 0.8349\n",
            "Emotion generation success: 90.08%\n",
            "Epoch time: 1141.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1431: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
            "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_1)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 3.9118\n",
            "Average quality: 0.3371\n",
            "Average emotion: 2.3640\n",
            "Average relevance: 0.3865\n",
            "Average sentiment: 0.3459\n",
            "Average empathy: 0.8476\n",
            "Emotion generation success: 94.38%\n",
            "Validation time: 124.1s\n",
            "\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:05<21:52,  5.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=7.820, Quality=2.000, Emotion=3.000, Relevance=2.125, Sentiment=0.077, Empathy=0.861, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [00:33<20:01,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=3.235, Quality=1.250, Emotion=1.750, Relevance=-0.375, Sentiment=0.241, Empathy=0.843, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [00:58<19:09,  5.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=3.082, Quality=-0.250, Emotion=1.688, Relevance=0.812, Sentiment=0.344, Empathy=0.846, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [01:25<19:29,  5.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=1.122, Quality=-2.500, Emotion=2.375, Relevance=-0.312, Sentiment=0.334, Empathy=0.852, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [01:51<18:41,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=5.787, Quality=1.250, Emotion=2.375, Relevance=1.438, Sentiment=0.142, Empathy=0.841, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [02:19<19:16,  5.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=1.060, Quality=-1.000, Emotion=1.188, Relevance=0.125, Sentiment=0.418, Empathy=0.865, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [02:44<16:47,  4.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=4.142, Quality=2.000, Emotion=3.000, Relevance=-1.625, Sentiment=0.327, Empathy=0.835, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [03:10<16:44,  5.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=4.696, Quality=0.500, Emotion=2.375, Relevance=1.250, Sentiment=0.330, Empathy=0.848, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [03:35<16:49,  5.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=2.124, Quality=-1.000, Emotion=1.812, Relevance=-0.562, Sentiment=0.245, Empathy=0.823, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [04:05<17:29,  5.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=1.910, Quality=-0.750, Emotion=2.375, Relevance=-0.875, Sentiment=0.320, Empathy=0.801, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [04:33<17:08,  5.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=2.770, Quality=-1.250, Emotion=2.188, Relevance=0.875, Sentiment=0.310, Empathy=0.865, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [04:57<15:05,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=6.062, Quality=1.250, Emotion=3.000, Relevance=1.125, Sentiment=0.326, Empathy=0.861, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [05:23<14:29,  5.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=6.137, Quality=1.250, Emotion=3.000, Relevance=1.188, Sentiment=0.297, Empathy=0.756, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [05:55<16:56,  6.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=1.659, Quality=-1.000, Emotion=2.375, Relevance=-0.625, Sentiment=0.299, Empathy=0.864, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [06:19<13:55,  5.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=3.528, Quality=0.500, Emotion=3.000, Relevance=-0.938, Sentiment=0.500, Empathy=0.849, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [06:47<14:22,  5.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=1.240, Quality=-1.500, Emotion=3.000, Relevance=-1.125, Sentiment=0.315, Empathy=0.855, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [07:13<12:58,  5.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=4.069, Quality=1.250, Emotion=1.625, Relevance=0.312, Sentiment=0.151, Empathy=0.826, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [07:39<12:48,  5.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=4.276, Quality=0.500, Emotion=3.000, Relevance=-0.062, Sentiment=0.491, Empathy=0.852, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [08:06<12:32,  5.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=6.188, Quality=2.000, Emotion=2.625, Relevance=1.500, Sentiment=0.194, Empathy=0.868, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [08:33<12:05,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=1.309, Quality=0.500, Emotion=0.500, Relevance=0.062, Sentiment=0.222, Empathy=0.854, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [09:00<12:08,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=3.235, Quality=1.250, Emotion=1.562, Relevance=-0.500, Sentiment=0.247, Empathy=0.867, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [09:25<10:49,  5.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=4.102, Quality=-0.250, Emotion=3.000, Relevance=0.375, Sentiment=0.239, Empathy=0.857, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [09:49<10:11,  5.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=6.782, Quality=2.000, Emotion=2.375, Relevance=1.750, Sentiment=0.265, Empathy=0.851, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [10:16<10:00,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=3.588, Quality=-0.250, Emotion=3.000, Relevance=-0.062, Sentiment=0.336, Empathy=0.835, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [10:43<10:29,  5.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=4.270, Quality=0.500, Emotion=1.875, Relevance=1.125, Sentiment=0.380, Empathy=0.831, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [11:08<09:11,  5.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=3.950, Quality=0.250, Emotion=2.188, Relevance=1.188, Sentiment=0.304, Empathy=0.867, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [11:33<08:26,  4.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=4.047, Quality=2.000, Emotion=1.188, Relevance=-0.125, Sentiment=0.393, Empathy=0.846, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [11:57<07:59,  4.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=5.882, Quality=0.500, Emotion=3.000, Relevance=1.500, Sentiment=0.521, Empathy=0.851, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [12:22<07:23,  4.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=4.821, Quality=2.000, Emotion=2.375, Relevance=-0.312, Sentiment=0.392, Empathy=0.813, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [12:48<07:38,  5.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=7.377, Quality=1.250, Emotion=3.000, Relevance=2.500, Sentiment=0.259, Empathy=0.851, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [13:16<07:32,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=6.678, Quality=1.250, Emotion=3.000, Relevance=1.375, Sentiment=0.407, Empathy=0.862, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [13:42<06:59,  5.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=4.491, Quality=-1.000, Emotion=3.000, Relevance=1.750, Sentiment=0.316, Empathy=0.863, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [14:13<06:54,  5.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=3.795, Quality=0.000, Emotion=2.375, Relevance=0.875, Sentiment=0.315, Empathy=0.861, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [14:39<06:17,  5.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=3.820, Quality=-0.250, Emotion=1.562, Relevance=0.500, Sentiment=0.369, Empathy=0.807, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [15:06<05:14,  5.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=6.399, Quality=2.000, Emotion=2.375, Relevance=1.250, Sentiment=0.286, Empathy=0.859, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [15:33<05:20,  5.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=1.765, Quality=-0.750, Emotion=1.750, Relevance=-0.625, Sentiment=0.473, Empathy=0.857, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [16:00<04:47,  5.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=3.897, Quality=-0.250, Emotion=2.375, Relevance=0.875, Sentiment=0.598, Empathy=0.869, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [16:24<03:42,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=6.024, Quality=1.250, Emotion=3.000, Relevance=1.000, Sentiment=0.461, Empathy=0.859, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [16:51<03:42,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=3.725, Quality=1.250, Emotion=2.438, Relevance=-0.812, Sentiment=0.499, Empathy=0.857, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [17:16<03:08,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=5.059, Quality=-0.250, Emotion=3.000, Relevance=1.562, Sentiment=0.257, Empathy=0.865, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [17:43<02:56,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=6.576, Quality=0.500, Emotion=3.000, Relevance=2.250, Sentiment=0.133, Empathy=0.881, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [18:07<02:15,  5.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=6.505, Quality=1.250, Emotion=3.000, Relevance=1.375, Sentiment=0.379, Empathy=0.868, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [18:33<01:54,  5.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=6.527, Quality=0.500, Emotion=3.000, Relevance=2.375, Sentiment=0.145, Empathy=0.868, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [19:02<01:36,  5.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=3.106, Quality=-1.000, Emotion=2.438, Relevance=1.375, Sentiment=0.319, Empathy=0.791, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [19:31<01:13,  6.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=3.528, Quality=0.500, Emotion=1.750, Relevance=0.250, Sentiment=0.344, Empathy=0.834, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [20:01<00:41,  5.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=4.808, Quality=-0.250, Emotion=3.000, Relevance=1.750, Sentiment=0.268, Empathy=0.859, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [20:33<00:12,  6.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=7.529, Quality=1.250, Emotion=3.000, Relevance=2.438, Sentiment=0.368, Empathy=0.830, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [20:40<00:00,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 2 Results:\n",
            "Average total reward: 4.2435\n",
            "Average quality score: 0.4413\n",
            "Average emotion score: 2.3848\n",
            "Average relevance score: 0.6426\n",
            "Average sentiment score: 0.3234\n",
            "Average empathy score: 0.8526\n",
            "Emotion generation success: 94.92%\n",
            "Epoch time: 1241.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_2)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 4.5232\n",
            "Average quality: 0.4090\n",
            "Average emotion: 2.5461\n",
            "Average relevance: 0.8966\n",
            "Average sentiment: 0.2832\n",
            "Average empathy: 0.8568\n",
            "Emotion generation success: 97.08%\n",
            "Validation time: 135.7s\n",
            "\n",
            "Epoch 3/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:04<18:37,  4.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=8.002, Quality=2.000, Emotion=3.000, Relevance=1.938, Sentiment=0.343, Empathy=0.835, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [00:38<25:12,  6.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=1.497, Quality=-1.000, Emotion=1.750, Relevance=0.688, Sentiment=0.443, Empathy=0.855, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [01:07<22:17,  6.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=1.115, Quality=-1.750, Emotion=2.250, Relevance=0.000, Sentiment=0.187, Empathy=0.843, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [01:34<20:08,  5.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=2.057, Quality=-2.500, Emotion=2.375, Relevance=0.688, Sentiment=0.381, Empathy=0.816, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [02:04<20:42,  5.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=4.242, Quality=0.750, Emotion=1.188, Relevance=1.875, Sentiment=0.249, Empathy=0.865, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [02:31<18:46,  5.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=1.686, Quality=-3.000, Emotion=3.000, Relevance=1.062, Sentiment=0.457, Empathy=0.852, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [03:01<18:42,  5.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=7.818, Quality=1.250, Emotion=3.000, Relevance=2.750, Sentiment=0.567, Empathy=0.866, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [03:34<22:11,  6.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=4.406, Quality=-0.250, Emotion=1.750, Relevance=1.750, Sentiment=0.410, Empathy=0.880, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [04:01<17:07,  5.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=4.104, Quality=1.250, Emotion=1.188, Relevance=0.812, Sentiment=0.351, Empathy=0.861, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [04:34<17:53,  5.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=2.958, Quality=0.500, Emotion=2.375, Relevance=-0.500, Sentiment=0.296, Empathy=0.818, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [05:09<20:40,  6.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=2.647, Quality=-1.500, Emotion=1.562, Relevance=2.125, Sentiment=0.141, Empathy=0.866, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [05:39<17:23,  5.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=5.259, Quality=2.000, Emotion=2.438, Relevance=0.375, Sentiment=0.543, Empathy=0.846, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [06:13<18:38,  6.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=5.442, Quality=1.250, Emotion=3.000, Relevance=0.562, Sentiment=0.331, Empathy=0.844, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [06:46<17:13,  6.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=4.251, Quality=1.250, Emotion=1.750, Relevance=0.438, Sentiment=0.117, Empathy=0.851, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [07:13<14:55,  5.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=5.025, Quality=-0.250, Emotion=3.000, Relevance=1.750, Sentiment=0.248, Empathy=0.796, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [07:40<13:36,  5.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=3.457, Quality=0.500, Emotion=3.000, Relevance=-0.688, Sentiment=0.361, Empathy=0.845, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [08:09<14:19,  5.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=5.278, Quality=1.250, Emotion=2.188, Relevance=1.250, Sentiment=0.459, Empathy=0.856, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [08:40<14:24,  5.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=1.412, Quality=-1.250, Emotion=2.375, Relevance=-0.250, Sentiment=0.499, Empathy=0.821, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [09:08<13:36,  5.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=2.661, Quality=0.250, Emotion=1.438, Relevance=0.125, Sentiment=0.420, Empathy=0.867, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [09:35<12:09,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=4.860, Quality=1.000, Emotion=2.375, Relevance=0.938, Sentiment=0.278, Empathy=0.837, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [10:03<13:34,  6.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=3.030, Quality=-0.750, Emotion=2.250, Relevance=1.000, Sentiment=0.238, Empathy=0.863, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [10:28<11:11,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=4.806, Quality=-0.750, Emotion=2.375, Relevance=3.000, Sentiment=0.301, Empathy=0.847, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [11:00<13:02,  6.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=2.631, Quality=-1.000, Emotion=1.750, Relevance=1.125, Sentiment=0.316, Empathy=0.866, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [11:29<10:37,  5.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=6.367, Quality=1.000, Emotion=3.000, Relevance=1.438, Sentiment=0.399, Empathy=0.875, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [11:53<09:15,  4.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=6.332, Quality=1.250, Emotion=1.875, Relevance=3.438, Sentiment=0.201, Empathy=0.864, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [12:22<10:03,  5.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=3.604, Quality=-0.500, Emotion=2.188, Relevance=1.125, Sentiment=0.117, Empathy=0.845, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [12:51<10:36,  6.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=4.449, Quality=-0.250, Emotion=1.750, Relevance=2.312, Sentiment=0.377, Empathy=0.871, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [13:16<08:12,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=6.202, Quality=2.000, Emotion=2.375, Relevance=1.562, Sentiment=0.258, Empathy=0.879, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [13:43<07:46,  5.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=6.031, Quality=1.000, Emotion=3.000, Relevance=1.500, Sentiment=0.467, Empathy=0.819, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [14:10<07:38,  5.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=7.858, Quality=2.000, Emotion=3.000, Relevance=1.875, Sentiment=0.442, Empathy=0.854, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [14:44<08:39,  6.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=6.172, Quality=1.000, Emotion=2.375, Relevance=2.000, Sentiment=0.394, Empathy=0.862, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [15:11<07:32,  5.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=3.666, Quality=-0.250, Emotion=1.750, Relevance=1.250, Sentiment=0.191, Empathy=0.874, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [15:41<06:42,  5.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=3.916, Quality=0.500, Emotion=2.375, Relevance=0.562, Sentiment=0.281, Empathy=0.844, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [16:11<06:51,  6.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=3.231, Quality=-0.250, Emotion=2.188, Relevance=0.875, Sentiment=0.364, Empathy=0.854, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [16:37<05:23,  5.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=4.968, Quality=1.250, Emotion=2.375, Relevance=0.188, Sentiment=0.261, Empathy=0.855, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [17:05<05:18,  5.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=4.805, Quality=0.500, Emotion=2.375, Relevance=1.375, Sentiment=0.373, Empathy=0.868, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [17:30<04:45,  5.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=4.647, Quality=0.500, Emotion=1.750, Relevance=1.250, Sentiment=0.469, Empathy=0.870, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [17:54<03:51,  4.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=4.166, Quality=0.500, Emotion=2.375, Relevance=0.125, Sentiment=0.392, Empathy=0.859, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [18:26<04:22,  6.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=4.672, Quality=-0.250, Emotion=3.000, Relevance=1.562, Sentiment=0.225, Empathy=0.858, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [18:50<03:08,  5.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=5.491, Quality=1.250, Emotion=2.375, Relevance=0.875, Sentiment=0.253, Empathy=0.862, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [19:14<02:37,  4.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=4.191, Quality=0.500, Emotion=2.438, Relevance=0.625, Sentiment=0.280, Empathy=0.861, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [19:41<02:24,  5.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=4.458, Quality=-0.250, Emotion=2.375, Relevance=1.750, Sentiment=0.160, Empathy=0.876, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [20:10<02:10,  5.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=5.546, Quality=0.500, Emotion=2.375, Relevance=2.188, Sentiment=0.452, Empathy=0.874, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [20:38<01:37,  5.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=7.511, Quality=1.250, Emotion=2.375, Relevance=2.625, Sentiment=0.270, Empathy=0.859, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [21:06<01:09,  5.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=4.046, Quality=-1.000, Emotion=2.375, Relevance=1.625, Sentiment=0.192, Empathy=0.876, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [21:31<00:36,  5.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=4.349, Quality=0.500, Emotion=3.000, Relevance=0.125, Sentiment=0.500, Empathy=0.861, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [21:56<00:10,  5.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=6.419, Quality=1.250, Emotion=2.438, Relevance=1.438, Sentiment=0.268, Empathy=0.855, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [22:05<00:00,  5.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 3 Results:\n",
            "Average total reward: 4.6823\n",
            "Average quality score: 0.3714\n",
            "Average emotion score: 2.4319\n",
            "Average relevance score: 1.1422\n",
            "Average sentiment score: 0.3169\n",
            "Average empathy score: 0.8539\n",
            "Emotion generation success: 95.38%\n",
            "Epoch time: 1325.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_3)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 5.2135\n",
            "Average quality: 0.6697\n",
            "Average emotion: 2.4854\n",
            "Average relevance: 1.2966\n",
            "Average sentiment: 0.2698\n",
            "Average empathy: 0.8550\n",
            "Emotion generation success: 97.08%\n",
            "Validation time: 122.8s\n",
            "\n",
            "Epoch 4/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:07<28:35,  7.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=4.475, Quality=-0.250, Emotion=1.750, Relevance=2.062, Sentiment=0.233, Empathy=0.880, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [00:39<23:31,  6.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=8.122, Quality=1.750, Emotion=3.000, Relevance=2.438, Sentiment=0.523, Empathy=0.854, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [01:05<18:58,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=5.656, Quality=1.250, Emotion=2.812, Relevance=1.188, Sentiment=0.310, Empathy=0.873, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [01:32<18:32,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=6.641, Quality=2.000, Emotion=2.375, Relevance=1.312, Sentiment=0.261, Empathy=0.850, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [02:00<19:17,  5.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=4.329, Quality=-0.250, Emotion=3.000, Relevance=1.062, Sentiment=0.188, Empathy=0.857, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [02:26<17:43,  5.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=6.910, Quality=2.000, Emotion=3.000, Relevance=1.188, Sentiment=0.299, Empathy=0.847, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [02:54<17:25,  5.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=5.701, Quality=2.000, Emotion=2.375, Relevance=0.500, Sentiment=0.493, Empathy=0.854, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [03:25<18:22,  5.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=3.629, Quality=-0.250, Emotion=2.375, Relevance=0.875, Sentiment=0.439, Empathy=0.870, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [03:52<17:03,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=3.968, Quality=-0.250, Emotion=2.375, Relevance=1.375, Sentiment=0.202, Empathy=0.866, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [04:32<24:28,  7.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=2.083, Quality=-1.750, Emotion=2.438, Relevance=0.750, Sentiment=0.190, Empathy=0.866, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [05:06<20:24,  6.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=4.557, Quality=-0.250, Emotion=1.562, Relevance=2.250, Sentiment=0.366, Empathy=0.861, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [05:32<15:26,  5.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=4.775, Quality=0.500, Emotion=2.375, Relevance=1.000, Sentiment=0.281, Empathy=0.852, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [06:04<18:10,  6.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=2.958, Quality=-0.250, Emotion=1.250, Relevance=1.125, Sentiment=0.365, Empathy=0.875, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [06:35<17:44,  6.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=3.686, Quality=-1.000, Emotion=1.750, Relevance=1.562, Sentiment=0.159, Empathy=0.881, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [07:00<13:59,  5.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=6.178, Quality=1.250, Emotion=3.000, Relevance=1.375, Sentiment=0.277, Empathy=0.839, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [07:28<13:56,  5.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=6.570, Quality=1.250, Emotion=3.000, Relevance=1.312, Sentiment=0.341, Empathy=0.871, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [07:58<14:56,  5.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=4.331, Quality=-0.250, Emotion=2.812, Relevance=1.250, Sentiment=0.213, Empathy=0.847, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [08:25<13:19,  5.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=8.615, Quality=2.000, Emotion=3.000, Relevance=2.812, Sentiment=0.356, Empathy=0.821, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [08:51<12:28,  5.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=4.200, Quality=-0.250, Emotion=2.625, Relevance=1.062, Sentiment=0.308, Empathy=0.854, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [09:17<11:59,  5.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=6.398, Quality=0.250, Emotion=3.000, Relevance=1.938, Sentiment=0.378, Empathy=0.860, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [09:43<11:14,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=2.849, Quality=-1.000, Emotion=2.812, Relevance=0.438, Sentiment=0.368, Empathy=0.876, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [10:11<11:21,  5.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=6.936, Quality=1.250, Emotion=3.000, Relevance=2.062, Sentiment=0.473, Empathy=0.817, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [10:39<11:22,  5.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=6.093, Quality=1.250, Emotion=1.812, Relevance=2.250, Sentiment=0.220, Empathy=0.886, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [11:04<10:31,  5.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=5.072, Quality=-0.250, Emotion=2.375, Relevance=2.375, Sentiment=0.517, Empathy=0.865, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [11:29<09:34,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=4.895, Quality=0.500, Emotion=1.625, Relevance=2.312, Sentiment=0.508, Empathy=0.850, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [11:57<09:40,  5.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=4.892, Quality=1.250, Emotion=2.812, Relevance=0.562, Sentiment=0.177, Empathy=0.846, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [12:27<10:31,  6.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=4.111, Quality=-1.000, Emotion=3.000, Relevance=1.375, Sentiment=0.559, Empathy=0.856, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [12:55<09:14,  5.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=7.099, Quality=1.250, Emotion=3.000, Relevance=1.812, Sentiment=0.395, Empathy=0.877, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [13:27<10:06,  6.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=5.459, Quality=1.250, Emotion=2.375, Relevance=0.438, Sentiment=0.397, Empathy=0.854, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [13:56<08:35,  5.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=6.134, Quality=1.250, Emotion=1.812, Relevance=2.375, Sentiment=0.472, Empathy=0.866, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [14:26<08:17,  6.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=5.888, Quality=0.500, Emotion=3.000, Relevance=1.812, Sentiment=0.275, Empathy=0.867, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [14:54<07:24,  5.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=2.269, Quality=-1.750, Emotion=2.375, Relevance=1.125, Sentiment=0.260, Empathy=0.876, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [15:25<06:59,  5.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=3.763, Quality=-1.750, Emotion=3.000, Relevance=1.812, Sentiment=0.329, Empathy=0.870, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [15:54<06:49,  6.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=2.353, Quality=0.500, Emotion=0.938, Relevance=0.500, Sentiment=0.381, Empathy=0.857, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [16:21<05:42,  5.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=4.077, Quality=0.500, Emotion=1.750, Relevance=1.062, Sentiment=0.272, Empathy=0.866, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [16:49<05:40,  5.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=1.008, Quality=-3.250, Emotion=2.375, Relevance=1.188, Sentiment=0.387, Empathy=0.818, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [17:18<05:07,  5.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=3.880, Quality=0.500, Emotion=1.562, Relevance=1.000, Sentiment=0.445, Empathy=0.887, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [17:45<04:17,  5.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=5.546, Quality=2.000, Emotion=2.375, Relevance=0.125, Sentiment=0.343, Empathy=0.871, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [18:12<03:54,  5.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=3.860, Quality=1.250, Emotion=1.812, Relevance=0.188, Sentiment=0.138, Empathy=0.861, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [18:38<03:13,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=5.148, Quality=1.250, Emotion=2.375, Relevance=0.875, Sentiment=0.304, Empathy=0.868, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [19:10<03:19,  6.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=4.779, Quality=0.500, Emotion=1.188, Relevance=2.250, Sentiment=0.170, Empathy=0.843, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [19:38<02:27,  5.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=6.866, Quality=2.000, Emotion=3.000, Relevance=1.188, Sentiment=0.225, Empathy=0.843, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [20:05<02:00,  5.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=6.449, Quality=0.500, Emotion=3.000, Relevance=2.125, Sentiment=0.297, Empathy=0.878, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [20:36<01:44,  6.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=4.723, Quality=-1.000, Emotion=3.000, Relevance=2.125, Sentiment=0.249, Empathy=0.864, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [21:02<01:06,  5.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=2.494, Quality=-0.250, Emotion=2.375, Relevance=-0.250, Sentiment=0.337, Empathy=0.878, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [21:31<00:40,  5.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=4.380, Quality=-0.250, Emotion=1.875, Relevance=2.000, Sentiment=0.384, Empathy=0.856, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [21:58<00:10,  5.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=7.803, Quality=1.250, Emotion=3.000, Relevance=3.250, Sentiment=0.187, Empathy=0.860, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [22:07<00:00,  5.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 4 Results:\n",
            "Average total reward: 5.2109\n",
            "Average quality score: 0.4832\n",
            "Average emotion score: 2.4840\n",
            "Average relevance score: 1.5329\n",
            "Average sentiment score: 0.3186\n",
            "Average empathy score: 0.8612\n",
            "Emotion generation success: 96.35%\n",
            "Epoch time: 1328.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_4)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 5.4200\n",
            "Average quality: 0.7865\n",
            "Average emotion: 2.3213\n",
            "Average relevance: 1.7393\n",
            "Average sentiment: 0.2774\n",
            "Average empathy: 0.8572\n",
            "Emotion generation success: 96.18%\n",
            "Validation time: 141.1s\n",
            "\n",
            "Epoch 5/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:05<20:16,  5.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=6.198, Quality=1.250, Emotion=2.375, Relevance=1.688, Sentiment=0.284, Empathy=0.863, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [00:37<24:06,  6.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=4.668, Quality=0.250, Emotion=2.438, Relevance=1.125, Sentiment=0.216, Empathy=0.866, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [01:03<20:22,  5.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=3.141, Quality=-1.750, Emotion=2.188, Relevance=1.000, Sentiment=0.367, Empathy=0.795, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [01:33<22:07,  6.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=6.858, Quality=1.250, Emotion=2.375, Relevance=2.312, Sentiment=0.560, Empathy=0.848, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [02:03<20:49,  5.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=5.111, Quality=0.500, Emotion=2.375, Relevance=1.938, Sentiment=0.211, Empathy=0.848, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [02:29<17:56,  5.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=8.455, Quality=1.250, Emotion=3.000, Relevance=3.438, Sentiment=0.365, Empathy=0.860, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [03:04<21:53,  6.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=5.115, Quality=0.500, Emotion=3.000, Relevance=0.812, Sentiment=0.246, Empathy=0.870, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [03:42<22:23,  6.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=4.070, Quality=-0.250, Emotion=2.375, Relevance=1.375, Sentiment=0.339, Empathy=0.866, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [04:10<18:50,  5.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=5.082, Quality=0.500, Emotion=2.438, Relevance=1.750, Sentiment=0.095, Empathy=0.879, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [04:46<21:49,  7.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=3.646, Quality=-0.750, Emotion=1.812, Relevance=1.125, Sentiment=0.291, Empathy=0.860, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [05:22<21:59,  7.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=3.430, Quality=-0.250, Emotion=1.562, Relevance=0.750, Sentiment=0.244, Empathy=0.850, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [05:54<19:06,  6.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=3.646, Quality=-1.250, Emotion=3.000, Relevance=1.250, Sentiment=0.285, Empathy=0.822, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [06:28<19:46,  6.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=5.528, Quality=-0.250, Emotion=2.438, Relevance=2.875, Sentiment=0.254, Empathy=0.859, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [07:06<21:10,  7.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=2.124, Quality=-1.000, Emotion=0.562, Relevance=1.688, Sentiment=0.265, Empathy=0.876, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [07:37<17:32,  6.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=5.973, Quality=0.500, Emotion=2.438, Relevance=2.500, Sentiment=0.268, Empathy=0.874, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [08:06<14:45,  5.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=7.522, Quality=2.000, Emotion=3.000, Relevance=2.062, Sentiment=0.471, Empathy=0.847, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [08:45<17:15,  6.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=5.388, Quality=1.250, Emotion=2.812, Relevance=0.688, Sentiment=0.149, Empathy=0.848, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [09:22<18:28,  7.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=3.822, Quality=-1.250, Emotion=1.812, Relevance=2.562, Sentiment=0.385, Empathy=0.852, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [09:52<14:00,  5.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=4.804, Quality=1.250, Emotion=2.000, Relevance=0.312, Sentiment=0.448, Empathy=0.805, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [10:20<12:54,  5.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=4.234, Quality=-0.250, Emotion=1.750, Relevance=2.375, Sentiment=0.005, Empathy=0.871, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [10:49<13:25,  6.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=3.540, Quality=0.500, Emotion=2.188, Relevance=-0.375, Sentiment=0.453, Empathy=0.882, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [11:21<14:28,  6.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=4.270, Quality=-0.250, Emotion=2.438, Relevance=1.312, Sentiment=0.190, Empathy=0.847, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [12:02<16:54,  8.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=2.598, Quality=-2.500, Emotion=1.812, Relevance=2.625, Sentiment=0.105, Empathy=0.877, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [12:42<14:23,  7.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=6.477, Quality=1.250, Emotion=3.000, Relevance=1.500, Sentiment=0.440, Empathy=0.903, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [13:17<13:17,  7.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=7.409, Quality=1.250, Emotion=1.875, Relevance=3.562, Sentiment=0.255, Empathy=0.848, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [14:01<14:27,  8.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=3.078, Quality=0.000, Emotion=0.125, Relevance=2.250, Sentiment=0.131, Empathy=0.874, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [14:41<14:01,  8.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=7.589, Quality=2.000, Emotion=3.000, Relevance=1.875, Sentiment=0.285, Empathy=0.862, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [15:11<10:24,  6.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=5.490, Quality=2.000, Emotion=2.438, Relevance=0.625, Sentiment=0.378, Empathy=0.851, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [15:52<11:25,  7.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=2.524, Quality=-0.250, Emotion=0.562, Relevance=1.000, Sentiment=0.193, Empathy=0.805, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [16:47<15:41, 10.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=3.892, Quality=-0.250, Emotion=1.250, Relevance=2.062, Sentiment=0.255, Empathy=0.884, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [17:43<14:24, 10.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=5.628, Quality=1.250, Emotion=1.312, Relevance=2.312, Sentiment=0.319, Empathy=0.869, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [18:24<11:28,  8.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=2.991, Quality=-1.750, Emotion=1.812, Relevance=1.438, Sentiment=0.141, Empathy=0.868, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [19:11<10:25,  8.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=3.616, Quality=-1.000, Emotion=3.000, Relevance=1.000, Sentiment=0.164, Empathy=0.892, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [19:59<10:27,  9.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=4.298, Quality=0.500, Emotion=0.375, Relevance=1.812, Sentiment=0.510, Empathy=0.877, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [20:39<08:55,  8.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=1.796, Quality=-0.250, Emotion=0.000, Relevance=0.875, Sentiment=0.271, Empathy=0.870, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [21:13<06:52,  7.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=4.501, Quality=-0.250, Emotion=1.812, Relevance=1.562, Sentiment=0.482, Empathy=0.879, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [22:04<09:27, 10.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=1.660, Quality=-1.000, Emotion=1.312, Relevance=-0.438, Sentiment=0.348, Empathy=0.861, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [22:51<07:28,  9.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=3.652, Quality=-0.250, Emotion=1.812, Relevance=1.062, Sentiment=0.317, Empathy=0.894, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [23:30<05:04,  7.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=6.484, Quality=1.250, Emotion=2.375, Relevance=2.188, Sentiment=0.173, Empathy=0.853, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [24:17<05:54,  9.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=0.609, Quality=-1.000, Emotion=-0.562, Relevance=0.750, Sentiment=0.257, Empathy=0.871, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [25:09<05:22, 10.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=2.817, Quality=-0.250, Emotion=0.062, Relevance=0.750, Sentiment=0.271, Empathy=0.880, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [26:00<04:47, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=2.208, Quality=-1.000, Emotion=0.688, Relevance=1.000, Sentiment=0.135, Empathy=0.885, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [26:47<03:26,  9.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=4.033, Quality=0.500, Emotion=2.375, Relevance=0.938, Sentiment=0.086, Empathy=0.882, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [27:29<02:31,  8.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=3.088, Quality=0.500, Emotion=0.750, Relevance=1.250, Sentiment=0.606, Empathy=0.862, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [28:15<01:45,  8.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=2.447, Quality=1.250, Emotion=1.125, Relevance=-0.938, Sentiment=0.116, Empathy=0.867, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [29:03<01:01,  8.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=4.921, Quality=1.250, Emotion=2.438, Relevance=0.438, Sentiment=0.526, Empathy=0.881, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [29:48<00:19,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=2.695, Quality=-1.000, Emotion=-0.562, Relevance=2.375, Sentiment=0.219, Empathy=0.867, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [30:02<00:00,  7.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 5 Results:\n",
            "Average total reward: 4.2183\n",
            "Average quality score: 0.1328\n",
            "Average emotion score: 1.7804\n",
            "Average relevance score: 1.4004\n",
            "Average sentiment score: 0.3084\n",
            "Average empathy score: 0.8636\n",
            "Emotion generation success: 94.46%\n",
            "Epoch time: 1802.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_5)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 2.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 3.6016\n",
            "Average quality: -0.1124\n",
            "Average emotion: 1.1348\n",
            "Average relevance: 1.5348\n",
            "Average sentiment: 0.2811\n",
            "Average empathy: 0.8650\n",
            "Emotion generation success: 94.16%\n",
            "Validation time: 239.1s\n",
            "\n",
            "Epoch 6/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:13<50:28, 13.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=2.353, Quality=-1.750, Emotion=0.062, Relevance=2.000, Sentiment=0.186, Empathy=0.861, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [01:18<46:30, 12.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=4.165, Quality=0.500, Emotion=1.312, Relevance=1.062, Sentiment=0.323, Empathy=0.853, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [02:05<35:26,  9.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=3.933, Quality=1.250, Emotion=1.688, Relevance=-0.125, Sentiment=0.436, Empathy=0.867, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [02:53<34:29,  9.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=1.718, Quality=0.500, Emotion=0.000, Relevance=-0.125, Sentiment=0.307, Empathy=0.880, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   9%|▉         | 21/233 [03:46<39:44, 11.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=0.702, Quality=-1.750, Emotion=0.062, Relevance=1.438, Sentiment=0.078, Empathy=0.877, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [04:49<42:44, 12.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=0.275, Quality=-1.750, Emotion=-0.500, Relevance=2.312, Sentiment=0.230, Empathy=0.851, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [05:47<39:40, 11.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=2.304, Quality=0.000, Emotion=0.750, Relevance=0.500, Sentiment=0.519, Empathy=0.849, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [06:54<41:06, 12.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=1.287, Quality=-0.250, Emotion=-1.125, Relevance=2.000, Sentiment=0.114, Empathy=0.874, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 41/233 [07:54<39:51, 12.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=3.492, Quality=-0.250, Emotion=0.750, Relevance=1.625, Sentiment=0.337, Empathy=0.873, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [09:03<43:59, 14.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=-0.306, Quality=-0.250, Emotion=-1.062, Relevance=-0.688, Sentiment=0.196, Empathy=0.887, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [10:03<34:54, 11.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=3.662, Quality=-0.500, Emotion=1.000, Relevance=1.625, Sentiment=0.244, Empathy=0.869, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [11:05<32:31, 11.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=4.255, Quality=1.250, Emotion=1.125, Relevance=1.188, Sentiment=0.591, Empathy=0.862, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [11:56<27:56,  9.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=3.492, Quality=0.500, Emotion=1.250, Relevance=1.062, Sentiment=0.429, Empathy=0.838, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [13:03<37:32, 13.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=0.214, Quality=-1.750, Emotion=-1.188, Relevance=1.062, Sentiment=0.096, Empathy=0.891, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [13:58<27:21, 10.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=3.241, Quality=-0.250, Emotion=1.188, Relevance=1.562, Sentiment=0.367, Empathy=0.868, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [14:46<21:53,  8.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=4.688, Quality=1.250, Emotion=2.438, Relevance=0.250, Sentiment=0.319, Empathy=0.849, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [15:54<28:53, 11.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=4.398, Quality=0.500, Emotion=1.688, Relevance=1.812, Sentiment=0.378, Empathy=0.884, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [16:41<23:32,  9.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=2.845, Quality=-0.250, Emotion=1.188, Relevance=0.250, Sentiment=0.268, Empathy=0.884, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [17:22<19:27,  8.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=4.369, Quality=1.250, Emotion=2.000, Relevance=-0.250, Sentiment=0.227, Empathy=0.875, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [18:04<17:21,  7.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=4.912, Quality=0.500, Emotion=2.375, Relevance=1.438, Sentiment=0.350, Empathy=0.860, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [18:50<19:16,  8.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=2.873, Quality=-0.250, Emotion=1.250, Relevance=1.000, Sentiment=0.368, Empathy=0.875, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [19:47<23:06, 10.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=3.217, Quality=-0.250, Emotion=0.000, Relevance=1.688, Sentiment=0.268, Empathy=0.863, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [20:43<22:21, 11.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=4.230, Quality=0.500, Emotion=1.125, Relevance=1.562, Sentiment=0.245, Empathy=0.859, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [21:42<21:28, 11.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=4.190, Quality=1.250, Emotion=1.188, Relevance=0.250, Sentiment=0.230, Empathy=0.838, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [22:34<17:16,  9.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=5.942, Quality=2.000, Emotion=1.312, Relevance=2.562, Sentiment=0.192, Empathy=0.862, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [23:37<21:20, 11.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=-2.073, Quality=-3.250, Emotion=-0.875, Relevance=-0.062, Sentiment=0.135, Empathy=0.876, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [24:33<18:56, 11.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=5.917, Quality=0.500, Emotion=1.750, Relevance=2.438, Sentiment=0.367, Empathy=0.886, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [25:09<12:48,  7.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=4.906, Quality=0.500, Emotion=1.875, Relevance=1.688, Sentiment=0.381, Empathy=0.860, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [26:03<14:04,  9.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=5.769, Quality=1.250, Emotion=2.438, Relevance=1.375, Sentiment=0.251, Empathy=0.854, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [27:05<19:18, 13.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=1.599, Quality=-1.750, Emotion=-1.188, Relevance=2.375, Sentiment=0.116, Empathy=0.885, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [28:06<16:26, 12.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=3.668, Quality=0.500, Emotion=0.688, Relevance=1.562, Sentiment=0.186, Empathy=0.872, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [28:53<13:03, 10.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=3.541, Quality=-1.000, Emotion=1.750, Relevance=2.000, Sentiment=0.119, Empathy=0.857, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [29:44<12:07, 10.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=2.254, Quality=-0.750, Emotion=0.688, Relevance=0.625, Sentiment=0.196, Empathy=0.865, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [30:36<12:20, 11.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=3.348, Quality=-0.250, Emotion=0.625, Relevance=2.188, Sentiment=0.267, Empathy=0.862, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [31:13<07:51,  7.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=5.266, Quality=1.250, Emotion=3.000, Relevance=0.250, Sentiment=0.203, Empathy=0.868, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [32:03<08:36,  9.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=3.167, Quality=-0.500, Emotion=1.750, Relevance=0.500, Sentiment=0.374, Empathy=0.871, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [32:53<09:26, 10.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=2.905, Quality=-1.000, Emotion=-0.062, Relevance=0.688, Sentiment=0.476, Empathy=0.875, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [33:58<09:25, 12.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=4.831, Quality=0.500, Emotion=1.812, Relevance=0.875, Sentiment=0.344, Empathy=0.864, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [34:46<06:47,  9.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=4.929, Quality=0.500, Emotion=1.812, Relevance=0.312, Sentiment=0.310, Empathy=0.882, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [35:41<06:16, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=4.394, Quality=1.250, Emotion=1.812, Relevance=-0.438, Sentiment=0.239, Empathy=0.862, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [36:43<06:29, 12.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=-0.006, Quality=-3.750, Emotion=-1.188, Relevance=3.750, Sentiment=0.008, Empathy=0.886, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [37:43<05:28, 12.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=1.270, Quality=0.250, Emotion=-0.500, Relevance=0.000, Sentiment=0.368, Empathy=0.880, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [38:29<03:32,  9.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=4.274, Quality=-0.250, Emotion=1.250, Relevance=1.625, Sentiment=0.153, Empathy=0.825, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [39:16<02:46,  9.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=1.727, Quality=-1.000, Emotion=0.562, Relevance=0.250, Sentiment=0.200, Empathy=0.887, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [40:05<01:58,  9.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=4.706, Quality=1.250, Emotion=2.375, Relevance=-0.188, Sentiment=0.205, Empathy=0.869, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [40:50<01:04,  9.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=6.203, Quality=1.000, Emotion=1.750, Relevance=2.312, Sentiment=0.247, Empathy=0.898, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [41:37<00:21, 10.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=1.641, Quality=-1.000, Emotion=-0.562, Relevance=0.938, Sentiment=0.250, Empathy=0.901, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [41:46<00:00, 10.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 6 Results:\n",
            "Average total reward: 2.9685\n",
            "Average quality score: -0.1360\n",
            "Average emotion score: 0.6952\n",
            "Average relevance score: 1.0193\n",
            "Average sentiment score: 0.2865\n",
            "Average empathy score: 0.8702\n",
            "Emotion generation success: 87.53%\n",
            "Epoch time: 2506.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_6)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 3.3543\n",
            "Average quality: 0.0494\n",
            "Average emotion: 0.9865\n",
            "Average relevance: 0.9393\n",
            "Average sentiment: 0.3031\n",
            "Average empathy: 0.8729\n",
            "Emotion generation success: 86.07%\n",
            "Validation time: 238.5s\n",
            "\n",
            "Epoch 7/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:14<54:15, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=1.263, Quality=-1.750, Emotion=-0.688, Relevance=0.812, Sentiment=0.286, Empathy=0.832, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [01:19<49:18, 13.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=2.721, Quality=-1.000, Emotion=0.562, Relevance=0.062, Sentiment=0.205, Empathy=0.866, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▍         | 11/233 [02:08<36:11,  9.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=5.344, Quality=1.250, Emotion=2.812, Relevance=0.625, Sentiment=0.449, Empathy=0.872, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   7%|▋         | 16/233 [02:56<36:01,  9.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=2.770, Quality=-1.000, Emotion=0.500, Relevance=1.250, Sentiment=0.063, Empathy=0.887, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   8%|▊         | 18/233 [03:20<40:37, 11.34s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (40.05) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (829.25) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   8%|▊         | 19/233 [03:31<40:01, 11.22s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (317.10) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (23373.52) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   9%|▉         | 21/233 [03:52<38:29, 10.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=3.610, Quality=0.500, Emotion=0.562, Relevance=1.688, Sentiment=0.287, Empathy=0.868, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [04:48<37:57, 11.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=4.145, Quality=0.500, Emotion=0.688, Relevance=2.312, Sentiment=0.361, Empathy=0.869, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  13%|█▎        | 31/233 [05:47<37:06, 11.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=3.486, Quality=1.000, Emotion=0.562, Relevance=1.125, Sentiment=0.320, Empathy=0.848, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  15%|█▌        | 36/233 [06:51<39:50, 12.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=2.888, Quality=-1.000, Emotion=-0.062, Relevance=1.375, Sentiment=0.245, Empathy=0.887, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  17%|█▋        | 40/233 [07:35<37:28, 11.65s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (23.87) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (720.31) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  18%|█▊        | 41/233 [07:48<38:34, 12.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=3.038, Quality=-0.750, Emotion=0.000, Relevance=2.500, Sentiment=0.398, Empathy=0.866, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  20%|█▉        | 46/233 [08:57<45:16, 14.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=-2.616, Quality=-3.250, Emotion=-1.938, Relevance=-1.062, Sentiment=0.213, Empathy=0.886, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [10:05<41:15, 13.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=1.960, Quality=-1.750, Emotion=-0.250, Relevance=1.188, Sentiment=0.290, Empathy=0.906, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [11:02<33:10, 11.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=2.482, Quality=0.500, Emotion=1.125, Relevance=-0.500, Sentiment=0.266, Empathy=0.876, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  26%|██▌       | 61/233 [12:04<33:02, 11.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=1.640, Quality=-1.000, Emotion=1.188, Relevance=0.125, Sentiment=0.415, Empathy=0.871, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (102.79) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (3231.72) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  28%|██▊       | 66/233 [13:07<36:11, 13.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=-1.132, Quality=-3.750, Emotion=-1.312, Relevance=-0.375, Sentiment=0.131, Empathy=0.916, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [13:54<27:35, 10.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=3.557, Quality=-1.000, Emotion=0.500, Relevance=1.438, Sentiment=0.322, Empathy=0.890, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [14:44<23:08,  8.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=2.125, Quality=1.250, Emotion=1.125, Relevance=-0.688, Sentiment=0.296, Empathy=0.856, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [15:50<27:49, 10.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=6.586, Quality=2.000, Emotion=2.188, Relevance=1.562, Sentiment=0.499, Empathy=0.811, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [16:43<25:04, 10.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=1.016, Quality=-0.250, Emotion=-0.062, Relevance=0.188, Sentiment=0.553, Empathy=0.892, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [17:41<24:10, 10.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=4.340, Quality=0.500, Emotion=1.375, Relevance=1.688, Sentiment=0.259, Empathy=0.884, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 95/233 [18:16<20:29,  8.91s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (11.50) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  41%|████      | 96/233 [18:27<21:21,  9.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=3.982, Quality=-0.500, Emotion=1.125, Relevance=1.562, Sentiment=0.191, Empathy=0.886, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [19:22<22:59, 10.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=2.601, Quality=-0.250, Emotion=0.375, Relevance=-0.500, Sentiment=0.515, Empathy=0.815, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [20:21<24:42, 11.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=2.992, Quality=-0.250, Emotion=0.562, Relevance=0.000, Sentiment=0.206, Empathy=0.880, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [21:26<25:39, 12.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=-0.131, Quality=-2.500, Emotion=-1.375, Relevance=-0.125, Sentiment=0.184, Empathy=0.898, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [22:41<31:27, 16.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=-2.045, Quality=-4.000, Emotion=-1.938, Relevance=-0.250, Sentiment=0.294, Empathy=0.918, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [23:39<21:59, 11.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=3.380, Quality=-0.250, Emotion=0.875, Relevance=1.688, Sentiment=0.248, Empathy=0.884, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [24:46<22:32, 12.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=0.706, Quality=-1.000, Emotion=-0.938, Relevance=0.250, Sentiment=0.198, Empathy=0.867, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [25:48<20:41, 12.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=4.720, Quality=1.250, Emotion=1.875, Relevance=1.250, Sentiment=0.388, Empathy=0.871, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [26:28<14:06,  8.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=2.389, Quality=-0.250, Emotion=-0.125, Relevance=1.312, Sentiment=0.417, Empathy=0.877, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [27:25<16:39, 10.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=0.059, Quality=-0.250, Emotion=-0.062, Relevance=-1.250, Sentiment=0.307, Empathy=0.887, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [28:25<17:41, 12.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=0.129, Quality=-3.250, Emotion=-0.750, Relevance=0.875, Sentiment=0.265, Empathy=0.892, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [29:25<16:49, 12.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=2.197, Quality=-1.750, Emotion=-0.688, Relevance=2.188, Sentiment=0.173, Empathy=0.885, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [30:10<13:03, 10.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=2.853, Quality=-1.000, Emotion=0.500, Relevance=1.562, Sentiment=0.165, Empathy=0.866, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [31:18<15:45, 13.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=0.139, Quality=-1.750, Emotion=-1.312, Relevance=1.000, Sentiment=0.279, Empathy=0.895, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [32:12<12:50, 11.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=1.989, Quality=-1.750, Emotion=0.375, Relevance=0.500, Sentiment=0.275, Empathy=0.842, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [33:11<12:47, 12.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=-0.327, Quality=-1.000, Emotion=-0.062, Relevance=-1.375, Sentiment=0.290, Empathy=0.872, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [34:11<11:42, 12.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=-0.637, Quality=-2.500, Emotion=-0.688, Relevance=-0.062, Sentiment=0.313, Empathy=0.898, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▋  | 178/233 [34:29<09:48, 10.71s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (18.23) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  78%|███████▊  | 181/233 [35:10<11:02, 12.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=1.074, Quality=-1.750, Emotion=-1.312, Relevance=1.938, Sentiment=0.340, Empathy=0.872, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [36:16<10:39, 13.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=-1.479, Quality=-1.750, Emotion=-0.625, Relevance=-1.188, Sentiment=0.285, Empathy=0.910, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [37:00<06:44,  9.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=5.283, Quality=1.250, Emotion=2.375, Relevance=0.375, Sentiment=0.244, Empathy=0.871, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▎ | 195/233 [37:49<07:18, 11.53s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (20.71) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  84%|████████▍ | 196/233 [38:04<07:45, 12.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=1.392, Quality=-2.250, Emotion=-0.625, Relevance=0.438, Sentiment=0.295, Empathy=0.830, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [39:10<07:27, 13.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=-2.965, Quality=-1.000, Emotion=-3.625, Relevance=-0.938, Sentiment=0.137, Empathy=0.899, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [40:21<06:36, 14.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=0.638, Quality=-1.750, Emotion=-0.750, Relevance=0.562, Sentiment=0.196, Empathy=0.907, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  90%|█████████ | 210/233 [41:04<04:28, 11.68s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (10.25) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (52.14) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  91%|█████████ | 211/233 [41:13<03:56, 10.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=5.837, Quality=1.250, Emotion=1.188, Relevance=2.000, Sentiment=0.362, Empathy=0.899, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [42:01<02:58, 10.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=0.277, Quality=-1.000, Emotion=-0.688, Relevance=0.625, Sentiment=0.199, Empathy=0.888, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [43:03<02:22, 11.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=4.450, Quality=-0.250, Emotion=1.750, Relevance=1.562, Sentiment=0.225, Empathy=0.884, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  96%|█████████▌| 224/233 [43:27<01:27,  9.76s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (13.71) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (486.02) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  97%|█████████▋| 226/233 [43:48<01:07,  9.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=6.271, Quality=1.750, Emotion=2.438, Relevance=1.562, Sentiment=0.431, Empathy=0.878, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [44:43<00:22, 11.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=0.653, Quality=-1.000, Emotion=-1.188, Relevance=-0.125, Sentiment=0.211, Empathy=0.840, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [45:03<00:00, 11.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 7 Results:\n",
            "Average total reward: 2.2426\n",
            "Average quality score: -0.7229\n",
            "Average emotion score: 0.2228\n",
            "Average relevance score: 0.8038\n",
            "Average sentiment score: 0.2801\n",
            "Average empathy score: 0.8762\n",
            "Emotion generation success: 78.66%\n",
            "Epoch time: 2703.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_7)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 2.4735\n",
            "Average quality: -0.5888\n",
            "Average emotion: 0.2292\n",
            "Average relevance: 0.6697\n",
            "Average sentiment: 0.2838\n",
            "Average empathy: 0.8815\n",
            "Emotion generation success: 76.40%\n",
            "Validation time: 285.8s\n",
            "\n",
            "Epoch 8/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   0%|          | 1/233 [00:13<51:46, 13.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0: Reward=1.975, Quality=-1.750, Emotion=-0.125, Relevance=0.562, Sentiment=0.081, Empathy=0.890, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   3%|▎         | 6/233 [01:19<49:19, 13.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 5: Reward=1.287, Quality=-1.000, Emotion=-0.125, Relevance=0.312, Sentiment=0.375, Empathy=0.845, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (11.88) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (149.91) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   3%|▎         | 8/233 [01:39<42:28, 11.33s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (53.68) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   5%|▍         | 11/233 [02:07<36:18,  9.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 10: Reward=4.110, Quality=0.500, Emotion=1.562, Relevance=1.375, Sentiment=0.560, Empathy=0.877, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   5%|▌         | 12/233 [02:25<44:18, 12.03s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (12.95) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   7%|▋         | 16/233 [03:09<43:54, 12.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 15: Reward=-0.888, Quality=-3.250, Emotion=-2.000, Relevance=-0.875, Sentiment=0.251, Empathy=0.908, Success=50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:   8%|▊         | 19/233 [03:55<51:53, 14.55s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (14.30) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:   9%|▉         | 21/233 [04:17<45:14, 12.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 20: Reward=3.618, Quality=-0.250, Emotion=1.125, Relevance=1.812, Sentiment=0.121, Empathy=0.873, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  11%|█         | 26/233 [05:15<38:44, 11.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 25: Reward=1.622, Quality=-1.000, Emotion=0.500, Relevance=1.188, Sentiment=0.300, Empathy=0.878, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (11.27) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  13%|█▎        | 31/233 [06:46<55:55, 16.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 30: Reward=-0.152, Quality=-2.500, Emotion=-1.375, Relevance=1.000, Sentiment=0.164, Empathy=0.877, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining batches:  14%|█▎        | 32/233 [07:04<56:59, 17.01s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (12.92) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (146.10) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  15%|█▌        | 36/233 [07:48<40:53, 12.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 35: Reward=2.610, Quality=-1.000, Emotion=0.562, Relevance=1.438, Sentiment=0.208, Empathy=0.889, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  17%|█▋        | 40/233 [08:32<36:13, 11.26s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (37.57) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (1006.91) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  18%|█▊        | 41/233 [08:43<36:17, 11.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 40: Reward=2.305, Quality=1.250, Emotion=-1.188, Relevance=1.438, Sentiment=0.379, Empathy=0.873, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  18%|█▊        | 43/233 [09:09<38:27, 12.15s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (55.57) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  19%|█▉        | 44/233 [09:25<41:59, 13.33s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (22.96) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (319.59) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  20%|█▉        | 46/233 [09:52<41:55, 13.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 45: Reward=-1.029, Quality=-3.250, Emotion=-1.312, Relevance=-0.312, Sentiment=0.384, Empathy=0.873, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  22%|██▏       | 51/233 [11:00<41:34, 13.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 50: Reward=1.917, Quality=-1.750, Emotion=-0.875, Relevance=0.875, Sentiment=0.282, Empathy=0.903, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 56/233 [11:53<32:34, 11.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 55: Reward=2.318, Quality=-0.250, Emotion=1.125, Relevance=-0.875, Sentiment=0.324, Empathy=0.892, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  24%|██▍       | 57/233 [12:06<34:04, 11.62s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (17.75) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  25%|██▍       | 58/233 [12:18<33:52, 11.61s/it]/usr/local/lib/python3.12/dist-packages/trl/trainer/ppo_trainer.py:1246: UserWarning: The average ratio of batch (38.81) exceeds threshold 10.00. Skipping batch.\n",
            "  warnings.warn(\n",
            "Training batches:  26%|██▌       | 61/233 [12:51<30:41, 10.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 60: Reward=5.185, Quality=0.500, Emotion=1.750, Relevance=1.938, Sentiment=0.564, Empathy=0.870, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  28%|██▊       | 66/233 [14:01<39:44, 14.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 65: Reward=-0.573, Quality=-3.250, Emotion=-1.938, Relevance=0.688, Sentiment=0.119, Empathy=0.884, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  30%|███       | 71/233 [15:00<29:19, 10.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 70: Reward=3.264, Quality=-0.250, Emotion=-0.062, Relevance=2.375, Sentiment=0.268, Empathy=0.891, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  33%|███▎      | 76/233 [16:02<28:48, 11.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 75: Reward=1.694, Quality=-1.750, Emotion=1.125, Relevance=0.250, Sentiment=0.255, Empathy=0.887, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  35%|███▍      | 81/233 [17:05<29:38, 11.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 80: Reward=3.502, Quality=-0.250, Emotion=1.000, Relevance=1.250, Sentiment=0.399, Empathy=0.875, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  37%|███▋      | 86/233 [17:56<24:00,  9.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 85: Reward=1.013, Quality=-1.000, Emotion=-0.125, Relevance=0.188, Sentiment=0.260, Empathy=0.883, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  39%|███▉      | 91/233 [18:48<21:58,  9.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 90: Reward=1.983, Quality=0.500, Emotion=0.812, Relevance=-0.125, Sentiment=0.126, Empathy=0.880, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  41%|████      | 96/233 [19:38<23:27, 10.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 95: Reward=0.594, Quality=-2.500, Emotion=-0.125, Relevance=1.375, Sentiment=0.216, Empathy=0.909, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  43%|████▎     | 101/233 [20:41<24:12, 11.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 100: Reward=5.160, Quality=0.500, Emotion=1.562, Relevance=0.750, Sentiment=0.298, Empathy=0.884, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  45%|████▌     | 106/233 [21:47<26:14, 12.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 105: Reward=3.189, Quality=-1.000, Emotion=1.188, Relevance=0.375, Sentiment=0.272, Empathy=0.812, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  48%|████▊     | 111/233 [22:52<27:42, 13.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 110: Reward=-0.191, Quality=-3.250, Emotion=-2.000, Relevance=0.938, Sentiment=0.139, Empathy=0.898, Success=50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  50%|████▉     | 116/233 [23:59<26:33, 13.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 115: Reward=3.008, Quality=-0.250, Emotion=-0.062, Relevance=0.062, Sentiment=0.259, Empathy=0.867, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  52%|█████▏    | 121/233 [24:46<17:32,  9.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 120: Reward=5.881, Quality=1.250, Emotion=1.875, Relevance=2.125, Sentiment=0.360, Empathy=0.869, Success=100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  54%|█████▍    | 126/233 [25:53<21:15, 11.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 125: Reward=0.881, Quality=-1.750, Emotion=-0.250, Relevance=1.375, Sentiment=0.278, Empathy=0.891, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  56%|█████▌    | 131/233 [27:04<22:55, 13.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 130: Reward=3.541, Quality=-0.250, Emotion=-0.062, Relevance=2.125, Sentiment=0.379, Empathy=0.880, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  58%|█████▊    | 136/233 [27:51<15:54,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 135: Reward=3.006, Quality=-0.250, Emotion=0.500, Relevance=1.375, Sentiment=0.134, Empathy=0.859, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  61%|██████    | 141/233 [28:40<15:03,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 140: Reward=2.695, Quality=-0.250, Emotion=0.500, Relevance=-0.312, Sentiment=0.124, Empathy=0.881, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  63%|██████▎   | 146/233 [29:37<16:19, 11.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 145: Reward=0.580, Quality=-2.500, Emotion=-0.750, Relevance=0.062, Sentiment=0.183, Empathy=0.901, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  65%|██████▍   | 151/233 [30:38<17:30, 12.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 150: Reward=1.763, Quality=-1.750, Emotion=-0.750, Relevance=1.562, Sentiment=0.140, Empathy=0.888, Success=62.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  67%|██████▋   | 156/233 [31:28<13:41, 10.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 155: Reward=6.058, Quality=0.500, Emotion=1.750, Relevance=3.000, Sentiment=0.079, Empathy=0.885, Success=87.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  69%|██████▉   | 161/233 [32:22<11:58,  9.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 160: Reward=2.883, Quality=-0.250, Emotion=0.500, Relevance=0.188, Sentiment=0.170, Empathy=0.862, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  71%|███████   | 166/233 [33:21<12:34, 11.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 165: Reward=1.064, Quality=-0.250, Emotion=0.312, Relevance=-0.062, Sentiment=0.186, Empathy=0.880, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  73%|███████▎  | 171/233 [34:07<10:06,  9.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 170: Reward=0.333, Quality=-1.750, Emotion=0.500, Relevance=-0.938, Sentiment=0.417, Empathy=0.882, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  76%|███████▌  | 176/233 [35:00<09:49, 10.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 175: Reward=1.336, Quality=-0.250, Emotion=-0.688, Relevance=-0.188, Sentiment=0.278, Empathy=0.875, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  78%|███████▊  | 181/233 [36:00<10:49, 12.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 180: Reward=1.795, Quality=-2.000, Emotion=-0.125, Relevance=-0.125, Sentiment=0.431, Empathy=0.903, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  80%|███████▉  | 186/233 [37:01<09:36, 12.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 185: Reward=1.191, Quality=-1.750, Emotion=0.500, Relevance=-1.062, Sentiment=0.337, Empathy=0.889, Success=75.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  82%|████████▏ | 191/233 [37:55<07:16, 10.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 190: Reward=3.884, Quality=1.250, Emotion=2.375, Relevance=-0.500, Sentiment=0.391, Empathy=0.864, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  84%|████████▍ | 196/233 [38:55<07:45, 12.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 195: Reward=-0.503, Quality=-1.000, Emotion=-1.375, Relevance=-1.250, Sentiment=0.134, Empathy=0.876, Success=56.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  86%|████████▋ | 201/233 [40:00<07:41, 14.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 200: Reward=-2.365, Quality=-4.750, Emotion=-2.562, Relevance=1.125, Sentiment=0.242, Empathy=0.906, Success=50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  88%|████████▊ | 206/233 [41:07<06:33, 14.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 205: Reward=-0.608, Quality=-1.000, Emotion=-1.250, Relevance=-1.750, Sentiment=0.276, Empathy=0.907, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  91%|█████████ | 211/233 [42:01<04:11, 11.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 210: Reward=2.047, Quality=-0.250, Emotion=1.125, Relevance=-0.625, Sentiment=0.155, Empathy=0.881, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  93%|█████████▎| 216/233 [42:53<02:59, 10.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 215: Reward=2.903, Quality=-0.250, Emotion=0.562, Relevance=0.250, Sentiment=0.461, Empathy=0.875, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  95%|█████████▍| 221/233 [43:47<02:12, 11.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 220: Reward=3.553, Quality=1.250, Emotion=1.812, Relevance=-0.312, Sentiment=0.134, Empathy=0.865, Success=93.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  97%|█████████▋| 226/233 [44:34<01:15, 10.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 225: Reward=-0.536, Quality=-3.750, Emotion=-0.125, Relevance=-0.375, Sentiment=0.282, Empathy=0.883, Success=68.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches:  99%|█████████▉| 231/233 [45:29<00:23, 11.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 230: Reward=4.283, Quality=-0.250, Emotion=0.562, Relevance=1.688, Sentiment=0.226, Empathy=0.887, Success=81.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training batches: 100%|██████████| 233/233 [45:41<00:00, 11.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 232: Batch size (16) does not match number of examples - but got 9 for: queries\n",
            "\n",
            "Epoch 8 Results:\n",
            "Average total reward: 1.8181\n",
            "Average quality score: -0.9153\n",
            "Average emotion score: -0.0516\n",
            "Average relevance score: 0.4733\n",
            "Average sentiment score: 0.2788\n",
            "Average empathy score: 0.8840\n",
            "Emotion generation success: 73.72%\n",
            "Epoch time: 2741.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_8)... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/epoch_8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results:\n",
            "Average reward: 2.1384\n",
            "Average quality: -0.6427\n",
            "Average emotion: -0.0180\n",
            "Average relevance: 0.3618\n",
            "Average sentiment: 0.2897\n",
            "Average empathy: 0.8862\n",
            "Emotion generation success: 73.93%\n",
            "Validation time: 297.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/final_model)... Done. 1.6s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed! Total time: 16402.2s\n",
            "Final model saved to /content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model/final_model\n",
            "W&B run url: https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>distributions/emotion_mean</td><td>▆███▆▃▂▁</td></tr><tr><td>distributions/emotion_std</td><td>▄▂▁▁▄▇██</td></tr><tr><td>distributions/empathy_mean</td><td>▁▄▄▅▅▆▇█</td></tr><tr><td>distributions/empathy_std</td><td>█▃▄▂▃▁▃▁</td></tr><tr><td>distributions/quality_mean</td><td>▇█▇█▆▅▂▁</td></tr><tr><td>distributions/quality_std</td><td>▂▁▂▁▃▅▇█</td></tr><tr><td>distributions/relevance_mean</td><td>▁▄▆█▇▆▅▃</td></tr><tr><td>distributions/relevance_std</td><td>▆▄▄▁▂▄▇█</td></tr><tr><td>distributions/reward_mean</td><td>▃▆▇█▆▃▂▁</td></tr><tr><td>distributions/reward_std</td><td>▄▂▂▁▃▆▇█</td></tr><tr><td>distributions/sentiment_mean</td><td>██▇▇▆▂▁▁</td></tr><tr><td>distributions/sentiment_std</td><td>██▇▇▆▂▂▁</td></tr><tr><td>distributions/val_emotion_mean</td><td>███▇▄▄▂▁</td></tr><tr><td>distributions/val_emotion_std</td><td>▂▁▁▂▆▆██</td></tr><tr><td>distributions/val_empathy_mean</td><td>▁▃▂▃▄▆▇█</td></tr><tr><td>distributions/val_empathy_std</td><td>▇▅█▆▃▄▃▁</td></tr><tr><td>distributions/val_quality_mean</td><td>▆▆▇█▄▄▁▁</td></tr><tr><td>distributions/val_quality_std</td><td>▄▃▂▁▆▅██</td></tr><tr><td>distributions/val_relevance_mean</td><td>▁▄▆█▇▄▃▁</td></tr><tr><td>distributions/val_relevance_std</td><td>▆▇▃▁▁▅▆█</td></tr><tr><td>distributions/val_reward_mean</td><td>▅▆██▄▄▂▁</td></tr><tr><td>distributions/val_reward_std</td><td>▃▂▁▁▆▆▇█</td></tr><tr><td>distributions/val_sentiment_mean</td><td>█▂▁▂▂▄▂▃</td></tr><tr><td>distributions/val_sentiment_std</td><td>█▃▁▃▂▅▂▃</td></tr><tr><td>epoch/emotion</td><td>▆███▆▃▂▁</td></tr><tr><td>epoch/emotion_success_rate</td><td>▆███▇▅▃▁</td></tr><tr><td>epoch/empathy</td><td>▁▄▄▅▅▆▇█</td></tr><tr><td>epoch/number</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>epoch/quality</td><td>▇█▇█▆▅▂▁</td></tr><tr><td>epoch/relevance</td><td>▁▄▆█▇▆▅▃</td></tr><tr><td>epoch/reward</td><td>▃▆▇█▆▃▂▁</td></tr><tr><td>epoch/sentiment</td><td>██▇▇▆▂▁▁</td></tr><tr><td>epoch/time_seconds</td><td>▁▁▂▂▄▇██</td></tr><tr><td>ppo/objective/entropy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▂▃▃▂▃▄▃▄▄▅▅▅▆▄█▃▄</td></tr><tr><td>ppo/objective/kl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/objective/kl_coef</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>ppo/ppo/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/loss/policy</td><td>▃▃▃▃▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂▂▁▂▁▂▂▁▂▁▂▂▂▃▂█▂▂▁▂▂</td></tr><tr><td>ppo/ppo/loss/total</td><td>▆█▅▄▃▃▂▃▃▇▃▂▃▂▃▂▃▂▃▂▄▂▅▃▃▄▄▂▂▃▃▃▂▁▃▃▁▂▁▂</td></tr><tr><td>ppo/ppo/loss/value</td><td>▇▃▅▃▅▃▅▃▆▄▃▂▁▂█▃▄▄▃▅▃▄▂▂▃▂▃▁▁▂▃▂▂▃▂▂▁▁▆▂</td></tr><tr><td>ppo/ppo/mean_non_score_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/ppo/mean_scores</td><td>▅▂▅▄▆▆▇▅▆▆▆▅▆▆▆▇█▇▇▇▇█▆▇██▂▅▄▅▅▅▂▂▅▃▄▆▁▃</td></tr><tr><td>ppo/ppo/policy/advantages_mean</td><td>▅▄▄▆██▅▅▅▅▅▅▃▅▂▄▃▅▄▄▅▅▅▅▄▄▄▄▄▄▅▄▆▄▁▄▂▂▁▅</td></tr><tr><td>ppo/ppo/policy/approxkl</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▃▁▁▁▃▁▁▁▁▁▁▁▁▂▁▃▂█▂▁▂▂</td></tr><tr><td>ppo/ppo/policy/clipfrac</td><td>▁▁▁▁▁▁▁▁▂▂▁▂▁▁▂▂▂▃▂▃▄▄█▇▅▄▄▆▇▂█▇▂▃▅▆▄▄▂▄</td></tr><tr><td>ppo/ppo/policy/entropy</td><td>▃▃▂▂▂▃▂▂▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▃▄▆▄▅▄▇▆▅▆▆▅█▇▇</td></tr><tr><td>ppo/ppo/policy/policykl</td><td>▄▄▄▄▄▅▄▄▄▄▄▄▇▄▆▄▇▅▄▅▅▅▅▆▃▆▆▅▂▄▄▃▄▃▁█▄▄▁▂</td></tr><tr><td>ppo/ppo/returns/mean</td><td>▅▇▆▅▇▆▇▇▇▆▆█▆▇▅▇█▇▇▇▆▇▆▆▆▇▆▅▆▆▄▃▂▂▂▂▁█▂▂</td></tr><tr><td>ppo/ppo/returns/var</td><td>▃▃▂▃▂▂▁▂▂▃▄▃▃▄▃▃▂▃▁▃▂▄▁▅▅▆▅▆▆▃▄▅▆▇▅█▇▇▆▆</td></tr><tr><td>ppo/ppo/std_scores</td><td>▅▃▄▄▃▅▆▄▅▄▇▄▄▄▅▄▂▇▅▂▄▃▅▅▃▆▇▇▆▄▁▄▅▄▆▇▆▇▇█</td></tr><tr><td>ppo/ppo/val/clipfrac</td><td>▂▁▁▁▁▁▁▁▂▁▂▁▁▄▁▁▄▁▄▁▂▅▂█▁▁▁▁▃▁▅▁▁▁▁▁▂▁▁▁</td></tr><tr><td>ppo/ppo/val/error</td><td>█▂▃▂▂▃▄▄▂▃▂▂▄▁▁▂▄▃▁▁▁▂▃▄▂▄▅▂▁▂▃▂▂▂▂▁▂▂▁▂</td></tr><tr><td>ppo/ppo/val/mean</td><td>▆▆▇▆▇▆▇▇██▇▇▇▇▇█▇▆▅▇▇▅▄▃▂▃▂▂▂▁▃▂▃▆▂▂▂▂▁▁</td></tr><tr><td>ppo/ppo/val/var</td><td>▂▁▂▁▁▁▁▁▂▁▁▁▂▂▂▂▃▂▃▃▄▃▅▇▆▄▂▅▆▅▇▅▅▅▄▆▅▆█▇</td></tr><tr><td>ppo/ppo/val/var_explained</td><td>▆▃▆▆▆▅▆▇▇▇▇▆▇▆▇▇▇▇▅▁▇▇█▇█▆▇██████▇██▇███</td></tr><tr><td>ppo/ppo/val/vpred</td><td>▆▇▇▇▇▇▆▇▇▇▆█▇▇▅▅▆▄▄▃▃▂▄▃▃▆▃▂▃▃▂▃▃▂▂▁▂▁▂▁</td></tr><tr><td>ppo/time/ppo/calc_stats</td><td>▄▂▂▃▂▂▂▂▂▂▁▂▃▃▇█▂█▃▂██▂██▃██████████████</td></tr><tr><td>ppo/time/ppo/compute_advantages</td><td>▁▁▁▂▂▂▂▂▂▇▁▃▁▇▁▂▃▁▃▇█▇▇▇▇█▇▂▇▆▆▇▆▆▆▇▇█▇▇</td></tr><tr><td>ppo/time/ppo/compute_rewards</td><td>▁▂▃▂▃▂▃▂▂▂▃▂▂▃▂▂▄▂▂▂▄▂▃▂▂▄▂▃█▂▃▃▂▃▁▂▃▂▂▂</td></tr><tr><td>ppo/time/ppo/forward_pass</td><td>▂▁▄▂▄▂▂▂▃▆▃▆▇▄▂▂▇▃▁▇▂▇▇▇▆▆▆█▇▆▇▆█▇█▆▇▇█▇</td></tr><tr><td>ppo/time/ppo/optimize_step</td><td>▃▂▁▂▁▂▂▁▁▁▁▂▂▂▂▂▃▁██▂█▂▂▃▇▂█████████████</td></tr><tr><td>ppo/time/ppo/total</td><td>▄▃▃▂█▁▂▂▂▁▂▁▂▃█▂▂▁██▁▁▂▁██▁█████████████</td></tr><tr><td>ppo/tokens/queries_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/tokens/queries_len_std</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>ppo/tokens/responses_len_mean</td><td>▁▂▁▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂▃▂▃▂▄▆▇▆▂▃▆▅▆▅▆▇▄█▇▆▇▇</td></tr><tr><td>ppo/tokens/responses_len_std</td><td>▁▁▁▁▁▁▂▆▁▁▁▁▁▂▁▅▄▁▁▁▁▁▁▇▄▇▁▇▂▆██▇█▇█▆█▄▄</td></tr><tr><td>train/batch</td><td>▂▂▂▅▆█▂▄▄▄█▄▅▆█▄▅▂▃▇█▁▂▂▃▄▆▆▇▇▁▃▄▄▅▆▇▇▂▇</td></tr><tr><td>train/batch_emotion</td><td>▆█▇█▇▅█▇▅▇▆▅██▇█▇▆█▇██▇▅▇█▇▅▆▃▅▅▆▆▂▆▄▁▅▃</td></tr><tr><td>train/batch_emotion_success_rate</td><td>▇████▆▆▇▆▅▃▇▆▇▆█▆██▇█▅█▇█▇▆▂▆▆▅▃▆▂▁▂▃▁▅▅</td></tr><tr><td>train/batch_empathy</td><td>▂▃▆▄▅▃▆▅▂▃▆▆▄▄▄▇▆▄▁▄▄▄▄▅▄▄▆▄▅▆█▄▄▆▅▅▇▆▆▄</td></tr><tr><td>train/batch_quality</td><td>▅▆▅▇▅▆▇▆▆▇▆▆▇█▅█▆▆▇▅▅▆▅▇▅▅█▄▆▅▇▅▅▃▅▄▆▃▁▄</td></tr><tr><td>train/batch_relevance</td><td>▁▅▄▄▃▆▇▇▇▆▅▅▆▄█▄▆▆▄▆▇▅▅▄▅▇▅▄▄▇▅▆▅▆▆▄▄▆▅▃</td></tr><tr><td>train/batch_reward</td><td>▆▆▄▅▆▄▆▄▇▆▅▇▇▅▅▆▅▇▇▇█▅▆▅▇▅▅▆▅▆▄▄▆▁▃▂▄▆▂▅</td></tr><tr><td>train/batch_sentiment</td><td>▂▆▆▄▇▄▃▆▆▅▁▄▅█▆▆▄▂▇▇▇▃▅▅▆▆▃▆▂▇▆▄▄▃▂▄▄▆▂▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>training/epochs_completed</td><td>▁</td></tr><tr><td>training/total_steps</td><td>▁</td></tr><tr><td>training/total_time_seconds</td><td>▁</td></tr><tr><td>val/emotion</td><td>███▇▄▄▂▁</td></tr><tr><td>val/emotion_success_rate</td><td>▇███▇▅▂▁</td></tr><tr><td>val/empathy</td><td>▁▃▂▃▄▆▇█</td></tr><tr><td>val/quality</td><td>▆▆▇█▄▄▁▁</td></tr><tr><td>val/relevance</td><td>▁▄▆█▇▄▃▁</td></tr><tr><td>val/reward</td><td>▅▆██▄▄▂▁</td></tr><tr><td>val/sentiment</td><td>█▂▁▂▂▄▂▃</td></tr><tr><td>val/time_seconds</td><td>▁▂▁▂▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>distributions/emotion_mean</td><td>-0.0516</td></tr><tr><td>distributions/emotion_std</td><td>4.53462</td></tr><tr><td>distributions/empathy_mean</td><td>0.88404</td></tr><tr><td>distributions/empathy_std</td><td>0.06453</td></tr><tr><td>distributions/quality_mean</td><td>-0.91535</td></tr><tr><td>distributions/quality_std</td><td>5.12793</td></tr><tr><td>distributions/relevance_mean</td><td>0.47326</td></tr><tr><td>distributions/relevance_std</td><td>4.17938</td></tr><tr><td>distributions/reward_mean</td><td>1.81805</td></tr><tr><td>distributions/reward_std</td><td>7.96925</td></tr><tr><td>distributions/sentiment_mean</td><td>0.27877</td></tr><tr><td>distributions/sentiment_std</td><td>0.41524</td></tr><tr><td>distributions/val_emotion_mean</td><td>-0.01798</td></tr><tr><td>distributions/val_emotion_std</td><td>4.52753</td></tr><tr><td>distributions/val_empathy_mean</td><td>0.88617</td></tr><tr><td>distributions/val_empathy_std</td><td>0.05483</td></tr><tr><td>distributions/val_quality_mean</td><td>-0.6427</td></tr><tr><td>distributions/val_quality_std</td><td>4.97278</td></tr><tr><td>distributions/val_relevance_mean</td><td>0.3618</td></tr><tr><td>distributions/val_relevance_std</td><td>4.14315</td></tr><tr><td>distributions/val_reward_mean</td><td>2.13844</td></tr><tr><td>distributions/val_reward_std</td><td>7.93719</td></tr><tr><td>distributions/val_sentiment_mean</td><td>0.28971</td></tr><tr><td>distributions/val_sentiment_std</td><td>0.4198</td></tr><tr><td>epoch/emotion</td><td>-0.0516</td></tr><tr><td>epoch/emotion_success_rate</td><td>0.73717</td></tr><tr><td>epoch/empathy</td><td>0.88404</td></tr><tr><td>epoch/number</td><td>8</td></tr><tr><td>epoch/quality</td><td>-0.91535</td></tr><tr><td>epoch/relevance</td><td>0.47326</td></tr><tr><td>epoch/reward</td><td>1.81805</td></tr><tr><td>epoch/sentiment</td><td>0.27877</td></tr><tr><td>epoch/time_seconds</td><td>2741.00345</td></tr><tr><td>ppo/objective/entropy</td><td>1221.81384</td></tr><tr><td>ppo/objective/kl</td><td>0</td></tr><tr><td>ppo/objective/kl_coef</td><td>0.11045</td></tr><tr><td>ppo/ppo/learning_rate</td><td>0.0</td></tr><tr><td>ppo/ppo/loss/policy</td><td>-0.02582</td></tr><tr><td>ppo/ppo/loss/total</td><td>0.14545</td></tr><tr><td>ppo/ppo/loss/value</td><td>1.71274</td></tr><tr><td>ppo/ppo/mean_non_score_reward</td><td>0</td></tr><tr><td>ppo/ppo/mean_scores</td><td>4.18946</td></tr><tr><td>ppo/ppo/policy/advantages_mean</td><td>0.0</td></tr><tr><td>ppo/ppo/policy/approxkl</td><td>0.27028</td></tr><tr><td>ppo/ppo/policy/clipfrac</td><td>0.28226</td></tr><tr><td>ppo/ppo/policy/entropy</td><td>3.30071</td></tr><tr><td>ppo/ppo/policy/policykl</td><td>-0.17011</td></tr><tr><td>ppo/ppo/returns/mean</td><td>-0.22791</td></tr><tr><td>ppo/ppo/returns/var</td><td>23.21604</td></tr><tr><td>ppo/ppo/std_scores</td><td>7.25134</td></tr><tr><td>ppo/ppo/val/clipfrac</td><td>0.07604</td></tr><tr><td>ppo/ppo/val/error</td><td>3.41258</td></tr><tr><td>ppo/ppo/val/mean</td><td>-1.32777</td></tr><tr><td>ppo/ppo/val/var</td><td>16.27418</td></tr><tr><td>ppo/ppo/val/var_explained</td><td>0.85301</td></tr><tr><td>ppo/ppo/val/vpred</td><td>-1.26789</td></tr><tr><td>ppo/time/ppo/calc_stats</td><td>0.13699</td></tr><tr><td>ppo/time/ppo/compute_advantages</td><td>0.02103</td></tr><tr><td>ppo/time/ppo/compute_rewards</td><td>0.00304</td></tr><tr><td>ppo/time/ppo/forward_pass</td><td>0.18034</td></tr><tr><td>ppo/time/ppo/optimize_step</td><td>0.86038</td></tr><tr><td>ppo/time/ppo/total</td><td>1.20195</td></tr><tr><td>ppo/tokens/queries_len_mean</td><td>128</td></tr><tr><td>ppo/tokens/queries_len_std</td><td>0</td></tr><tr><td>ppo/tokens/responses_len_mean</td><td>27.125</td></tr><tr><td>ppo/tokens/responses_len_std</td><td>39.70873</td></tr><tr><td>train/batch</td><td>231</td></tr><tr><td>train/batch_emotion</td><td>1.125</td></tr><tr><td>train/batch_emotion_success_rate</td><td>0.8125</td></tr><tr><td>train/batch_empathy</td><td>0.88951</td></tr><tr><td>train/batch_quality</td><td>0.5</td></tr><tr><td>train/batch_relevance</td><td>0.1875</td></tr><tr><td>train/batch_reward</td><td>4.18946</td></tr><tr><td>train/batch_sentiment</td><td>0.28808</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>training/epochs_completed</td><td>8</td></tr><tr><td>training/total_steps</td><td>1856</td></tr><tr><td>training/total_time_seconds</td><td>16402.20947</td></tr><tr><td>val/emotion</td><td>-0.01798</td></tr><tr><td>val/emotion_success_rate</td><td>0.73933</td></tr><tr><td>val/empathy</td><td>0.88617</td></tr><tr><td>val/quality</td><td>-0.6427</td></tr><tr><td>val/relevance</td><td>0.3618</td></tr><tr><td>val/reward</td><td>2.13844</td></tr><tr><td>val/sentiment</td><td>0.28971</td></tr><tr><td>val/time_seconds</td><td>297.75195</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">8m19therapy_ppo_enhanced_rewards</strong> at: <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced/runs/agsy9y6e</a><br> View project at: <a href='https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced' target=\"_blank\">https://wandb.ai/ericzhangez1006-ucl/8m19therapy-chatbot-ppo-enhanced</a><br>Synced 5 W&B file(s), 192 media file(s), 474 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250819_182837-agsy9y6e/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO training with enhanced reward components completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# ====================== Custom PPO Trainer ======================\n",
        "class CustomPPOTrainer(PPOTrainer):\n",
        "    \"\"\"Custom PPO Trainer that prevents data shuffling to maintain alignment\"\"\"\n",
        "\n",
        "    def prepare_dataloader(self, dataset, data_collator):\n",
        "        \"\"\"Override prepare_dataloader to disable shuffling\"\"\"\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=False,  # Disable shuffling\n",
        "            collate_fn=data_collator,\n",
        "            drop_last=False,\n",
        "            pin_memory=True,\n",
        "            num_workers=0,\n",
        "        )\n",
        "\n",
        "# ====================== Text Cleaning Functions ======================\n",
        "def clean_therapy_text(text):\n",
        "    \"\"\"Remove descriptive text patterns from SFT data\"\"\"\n",
        "    expression_pattern = r'\\s*(The (?:speaker|emotion state)[^.]*\\.(?:[^.]*\\.)*)'\n",
        "    match = re.search(expression_pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        return text[:match.start()].strip()\n",
        "    return text.strip()\n",
        "\n",
        "# ====================== Emotion Extraction ======================\n",
        "def extract_emotion_and_text_v4(full_response):\n",
        "    \"\"\"Extract emotion and text from SFT format: 'therapist_text <therapist_emotion> emotion_word <eos>'\"\"\"\n",
        "    full_response = full_response.strip()\n",
        "\n",
        "    if full_response.endswith('<eos>'):\n",
        "        full_response = full_response[:-5].strip()\n",
        "\n",
        "    emotion_pattern = r'<therapist_emotion>'\n",
        "    emotion_matches = list(re.finditer(emotion_pattern, full_response))\n",
        "\n",
        "    if emotion_matches:\n",
        "        last_emotion_match = emotion_matches[-1]\n",
        "        last_emotion_start = last_emotion_match.end()\n",
        "\n",
        "        emotion_part = full_response[last_emotion_start:].strip()\n",
        "\n",
        "        therapist_text = full_response[:last_emotion_match.start()].strip()\n",
        "        emotion_word = re.sub(r'<eos>.*$', '', emotion_part).strip().lower()\n",
        "\n",
        "        emotion_words = emotion_word.split()\n",
        "        if emotion_words:\n",
        "            emotion_word = emotion_words[0]\n",
        "\n",
        "        valid_emotions = {\"anger\", \"joy\", \"neutral\", \"sadness\", \"depression\", \"disgust\", \"fear\"}\n",
        "        if emotion_word in valid_emotions:\n",
        "            return therapist_text, emotion_word, True\n",
        "        else:\n",
        "            return therapist_text, emotion_word, True\n",
        "\n",
        "    return full_response, \"\", False\n",
        "\n",
        "# ====================== Advanced Reward Calculator ======================\n",
        "class TherapyRewardCalculator:\n",
        "    \"\"\"Enhanced reward calculator with sentiment and empathy models\"\"\"\n",
        "\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "        # 1. Sentiment classifier (distillBERT)\n",
        "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(device)\n",
        "\n",
        "        # 2. Empathy classifier (bert_empathy)\n",
        "        self.empathy_tokenizer = AutoTokenizer.from_pretrained(\"paragon-analytics/bert_empathy\")\n",
        "        self.empathy_model = AutoModelForSequenceClassification.from_pretrained(\"paragon-analytics/bert_empathy\").to(device)\n",
        "\n",
        "    def compute_empathy_reward(self, text):\n",
        "        \"\"\"Component 4: bert_empathy score\"\"\"\n",
        "        try:\n",
        "            inputs = self.empathy_tokenizer(text, return_tensors=\"pt\",\n",
        "                                          truncation=True, padding=True, max_length=512).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.empathy_model(**inputs)\n",
        "                probs = torch.softmax(outputs.logits, dim=-1)\n",
        "                empathy_score = probs[0][1].item()\n",
        "\n",
        "            return empathy_score\n",
        "        except:\n",
        "            return 0.5\n",
        "\n",
        "    def compute_sentiment_reward(self, text):\n",
        "        \"\"\"Component 2: distillBERT sentiment (positive sentiment reward)\"\"\"\n",
        "        try:\n",
        "            inputs = self.sentiment_tokenizer(text, return_tensors=\"pt\",\n",
        "                                            truncation=True, padding=True, max_length=512).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.sentiment_model(**inputs)\n",
        "                probs = torch.softmax(outputs.logits, dim=-1)\n",
        "                positive_score = probs[0][1].item()  \n",
        "\n",
        "            return positive_score\n",
        "        except:\n",
        "            return 0.5 \n",
        "\n",
        "# ====================== Dataset Class ======================\n",
        "class TherapyDatasetV4(Dataset):\n",
        "    \"\"\"Dataset class for 4thFIXED preprocessed data\"\"\"\n",
        "\n",
        "    def __init__(self, json_path, tokenizer_path=None):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            self._data = json.load(f)\n",
        "\n",
        "        if tokenizer_path and os.path.exists(tokenizer_path):\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "            print(f\"Loaded tokenizer from {tokenizer_path}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Tokenizer not found at {tokenizer_path}\")\n",
        "\n",
        "        self.bos_id = self.tokenizer.bos_token_id\n",
        "        self.eos_id = self.tokenizer.eos_token_id\n",
        "        self.pad_id = self.tokenizer.pad_token_id\n",
        "        self.problem_id = self.tokenizer.convert_tokens_to_ids(\"<problem>\")\n",
        "        self.user_id = self.tokenizer.convert_tokens_to_ids(\"<user>\")\n",
        "        self.user_emotion_id = self.tokenizer.convert_tokens_to_ids(\"<user_emotion>\")\n",
        "        self.therapist_id = self.tokenizer.convert_tokens_to_ids(\"<therapist>\")\n",
        "        self.therapist_emotion_id = self.tokenizer.convert_tokens_to_ids(\"<therapist_emotion>\")\n",
        "\n",
        "        self.max_length = 128\n",
        "        self.processed_data = []\n",
        "        self._preprocess_data_v4()\n",
        "\n",
        "    def _preprocess_data_v4(self):\n",
        "        \"\"\"Preprocess dialog data using 4thFIXED format\"\"\"\n",
        "        for conv in tqdm(self._data, desc=\"Processing data\"):\n",
        "            problem_type = conv.get(\"problem_type\", \"\").strip()\n",
        "            dialog = conv.get(\"dialog\", [])\n",
        "\n",
        "            user_text_parts = []\n",
        "            user_emotions = []\n",
        "\n",
        "            for turn in dialog:\n",
        "                speaker = turn.get(\"speaker\", \"\")\n",
        "                text = clean_therapy_text(turn.get(\"text\", \"\"))\n",
        "                emotion = turn.get(\"emotion\", \"\").strip()\n",
        "\n",
        "                if speaker != \"sys\":  # User turn\n",
        "                    if text:\n",
        "                        user_text_parts.append(text)\n",
        "                    if emotion:\n",
        "                        user_emotions.append(emotion)\n",
        "                else:  # Therapist turn\n",
        "                    if not user_text_parts:\n",
        "                        continue\n",
        "\n",
        "                    therapist_text = clean_therapy_text(text)\n",
        "                    therapist_emotion = emotion\n",
        "\n",
        "                    combined_user_text = \" \".join(user_text_parts)\n",
        "                    last_user_emotion = user_emotions[-1] if user_emotions else \"\"\n",
        "\n",
        "                    input_parts = []\n",
        "                    if problem_type:\n",
        "                        input_parts.append(f\"<problem>{problem_type}\")\n",
        "\n",
        "                    input_parts.append(f\"<user>{combined_user_text}\")\n",
        "                    if last_user_emotion:\n",
        "                        input_parts.append(f\"<user_emotion>{last_user_emotion}\")\n",
        "\n",
        "                    input_parts.append(\"<therapist>\")\n",
        "                    input_text = \" \".join(input_parts)\n",
        "\n",
        "                    input_encoding = self.tokenizer(\n",
        "                        input_text,\n",
        "                        max_length=self.max_length,\n",
        "                        padding='max_length',\n",
        "                        truncation=True,\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    self.processed_data.append({\n",
        "                        'input_ids': input_encoding['input_ids'].squeeze(),\n",
        "                        'attention_mask': input_encoding['attention_mask'].squeeze(),\n",
        "                        'query': input_text,\n",
        "                        'target_text': therapist_text,\n",
        "                        'target_emotion': therapist_emotion,\n",
        "                        'user_input': combined_user_text,\n",
        "                        'user_emotion': last_user_emotion\n",
        "                    })\n",
        "\n",
        "                    user_text_parts = []\n",
        "                    user_emotions = []\n",
        "\n",
        "        print(f\"Processed {len(self.processed_data)} therapist responses\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]\n",
        "\n",
        "    def to_hf_dataset(self):\n",
        "        \"\"\"Convert to HuggingFace Dataset format\"\"\"\n",
        "        return HFDataset.from_dict({\n",
        "            'input_ids': [x['input_ids'].tolist() for x in self.processed_data],\n",
        "            'attention_mask': [x['attention_mask'].tolist() for x in self.processed_data],\n",
        "            'query': [x['query'] for x in self.processed_data],\n",
        "            'target_text': [x['target_text'] for x in self.processed_data],\n",
        "            'target_emotion': [x['target_emotion'] for x in self.processed_data],\n",
        "            'user_input': [x['user_input'] for x in self.processed_data],\n",
        "            'user_emotion': [x['user_emotion'] for x in self.processed_data]\n",
        "        })\n",
        "\n",
        "# ====================== Generation Function ======================\n",
        "def generate_therapy_response(model, tokenizer, query_tensor, device, max_new_tokens=128):\n",
        "    \"\"\"Generate therapy response with conservative parameters\"\"\"\n",
        "\n",
        "    generation_kwargs = {\n",
        "        \"do_sample\": True,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 0.0,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            query_tensor.unsqueeze(0).to(device),\n",
        "            **generation_kwargs\n",
        "        )\n",
        "\n",
        "    generated_ids = outputs[0].cpu()\n",
        "    new_tokens = generated_ids[len(query_tensor):]\n",
        "\n",
        "    full_response = tokenizer.decode(\n",
        "        new_tokens.tolist(),\n",
        "        skip_special_tokens=False,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    ).strip()\n",
        "\n",
        "    therapist_text, therapist_emotion, has_emotion = extract_emotion_and_text_v4(full_response)\n",
        "\n",
        "    return new_tokens.to(device), therapist_text, therapist_emotion, has_emotion\n",
        "\n",
        "# ====================== ENHANCED Reward Functions ======================\n",
        "def detect_local_ngram_repetition(text, n=2, window_size=10, max_repeats=1, weight=3.0):\n",
        "    \"\"\"Enhanced repetition detection with stricter penalties\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < n:\n",
        "        return 0.0\n",
        "\n",
        "    penalty = 0.0\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        window = words[i:i + window_size]\n",
        "        ngrams = [tuple(window[j:j + n]) for j in range(len(window) - n + 1)]\n",
        "\n",
        "        ngram_counts = {}\n",
        "        for ngram in ngrams:\n",
        "            ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "\n",
        "        for ngram, count in ngram_counts.items():\n",
        "            if count > max_repeats:\n",
        "                penalty += (count - max_repeats) ** 1.5 * weight\n",
        "\n",
        "    return penalty\n",
        "\n",
        "def compute_text_quality_score_enhanced(response):\n",
        "    \"\"\"Text quality scoring with enhanced penalties for poor responses\"\"\"\n",
        "    if not response or not response.strip():\n",
        "        return -10.0  # Stricter penalty for empty responses\n",
        "\n",
        "    response_clean = response.strip()\n",
        "    words = response_clean.split()\n",
        "\n",
        "    base_score = 2.0  # Baseline score\n",
        "\n",
        "    # Enhanced repetition penalties\n",
        "    bigram_penalty = detect_local_ngram_repetition(response_clean, n=2, window_size=6, max_repeats=1, weight=4.0)\n",
        "    trigram_penalty = detect_local_ngram_repetition(response_clean, n=3, window_size=5, max_repeats=1, weight=6.0)\n",
        "\n",
        "    # Word-level repetition penalty\n",
        "    word_penalty = 0.0\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        clean_word = re.sub(r'[^\\w]', '', word.lower())\n",
        "        if len(clean_word) > 3:\n",
        "            word_counts[clean_word] = word_counts.get(clean_word, 0) + 1\n",
        "\n",
        "    for token, count in word_counts.items():\n",
        "        if count > 4:  \n",
        "            # Exponential penalty: more repeats = heavier punishment\n",
        "            penalty = min((count - 1) ** 1.7 * 1.5, 8.0)\n",
        "            word_penalty -= penalty\n",
        "\n",
        "    # Meaningless pattern detection (enhanced)\n",
        "    meaningless_patterns = [\n",
        "        r'^[^a-zA-Z]*$',      # No letters\n",
        "        r'^\\W+$',             # All symbols\n",
        "        r'^(.)\\1{4,}$',       # Repeated characters (lower threshold)\n",
        "        r'[^\\x00-\\x7F]{3,}',  # Non-standard characters (lower threshold)\n",
        "        r'\\b(?:yes|no|ok|thanks?)\\b',  # Simple responses\n",
        "        r'.{100,}'            # Too long responses\n",
        "    ]\n",
        "\n",
        "    for pattern in meaningless_patterns:\n",
        "        if re.search(pattern, response_clean, re.IGNORECASE):\n",
        "            return -10.0  # Stricter penalty for meaningless responses\n",
        "\n",
        "    quality_score = base_score - bigram_penalty - trigram_penalty + word_penalty\n",
        "    return max(min(quality_score, 8.0), -10.0)  # Extended negative range\n",
        "\n",
        "def compute_emotion_score_enhanced(has_emotion_token, generated_emotion, target_emotion):\n",
        "    \"\"\"Emotion scoring with enhanced rewards for non-neutral matches\"\"\"\n",
        "    if not has_emotion_token:\n",
        "        return -7.0  # Enhanced penalty for missing emotion tokens\n",
        "\n",
        "    valid_emotions = {\"anger\", \"joy\", \"neutral\", \"sadness\", \"depression\", \"disgust\", \"fear\"}\n",
        "    generated_emotion = generated_emotion.lower()\n",
        "\n",
        "    score = 0.0\n",
        "\n",
        "    # Stricter penalty for invalid emotions\n",
        "    if generated_emotion not in valid_emotions:\n",
        "        return -6.0\n",
        "\n",
        "    score += 1.0  # Base score for valid emotions\n",
        "\n",
        "    if not target_emotion:\n",
        "        return score\n",
        "\n",
        "    target_emotion = target_emotion.lower()\n",
        "\n",
        "    # Emotion group definitions\n",
        "    emotion_groups = {\n",
        "        \"negative\": {\"sadness\", \"depression\", \"anger\", \"disgust\", \"fear\"},\n",
        "        \"positive\": {\"joy\"},\n",
        "        \"neutral\": {\"neutral\"}\n",
        "    }\n",
        "\n",
        "    gen_group = next((g for g, e in emotion_groups.items() if generated_emotion in e), None)\n",
        "    target_group = next((g for g, e in emotion_groups.items() if target_emotion in e), None)\n",
        "\n",
        "    # Exact match rewards (enhanced for non-neutral emotions)\n",
        "    if generated_emotion == target_emotion:\n",
        "        if gen_group != \"neutral\":\n",
        "            return score + 6.0  \n",
        "        else:\n",
        "            return score + 1.0  # Neutral exact match reward\n",
        "\n",
        "    # Group match rewards (enhanced for non-neutral emotions)\n",
        "    if gen_group and target_group and gen_group == target_group:\n",
        "        if gen_group != \"neutral\":\n",
        "            return score + 3.0  # Non-neutral group match reward\n",
        "        else:\n",
        "            return score + 1.0  # Neutral group match reward\n",
        "\n",
        "    # Strict penalty for emotional conflicts\n",
        "    if (gen_group == \"positive\" and target_group == \"negative\") or \\\n",
        "       (gen_group == \"negative\" and target_group == \"positive\"):\n",
        "        return -5.0\n",
        "\n",
        "    return score - 1.0  # Base penalty for mismatches\n",
        "\n",
        "def compute_contextual_relevance_enhanced(generated_text, user_input, similarity_model, device):\n",
        "    \"\"\"Contextual relevance with enhanced penalties for irrelevance\"\"\"\n",
        "    if not generated_text or not user_input:\n",
        "        return -8.0  # Enhanced penalty for missing inputs\n",
        "\n",
        "    try:\n",
        "        # Text cleaning\n",
        "        gen_clean = re.sub(r'[^\\w\\s]', ' ', generated_text.lower()).strip()\n",
        "        user_clean = re.sub(r'[^\\w\\s]', ' ', user_input.lower()).strip()\n",
        "\n",
        "        if not gen_clean or not user_clean:\n",
        "            return -5.0\n",
        "\n",
        "        # Semantic similarity calculation\n",
        "        embeddings = similarity_model.encode([gen_clean, user_clean], convert_to_tensor=True)\n",
        "        embeddings = embeddings.to(device)\n",
        "        cos_sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
        "        similarity = float(cos_sim.item())\n",
        "\n",
        "        # Enhanced penalty mechanism\n",
        "        if similarity < 0.03:\n",
        "            return -10.0  \n",
        "        elif similarity < 0.05:\n",
        "            return -8.0\n",
        "        elif similarity < 0.08:\n",
        "            return -6.0\n",
        "        elif similarity < 0.12:\n",
        "            return -3.0\n",
        "        elif similarity < 0.20:\n",
        "            return 0.0\n",
        "        elif similarity < 0.30:\n",
        "            return 2.0\n",
        "        elif similarity < 0.50:\n",
        "            return 4.0\n",
        "        elif similarity < 0.70:\n",
        "            return 6.0\n",
        "        else:\n",
        "            return 8.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in relevance calculation: {e}\")\n",
        "        return -2.0  \n",
        "\n",
        "def compute_comprehensive_rewards_enhanced(\n",
        "    text_responses,\n",
        "    emotion_responses,\n",
        "    has_emotion_flags,\n",
        "    target_texts,\n",
        "    target_emotions,\n",
        "    user_inputs,\n",
        "    user_emotions,\n",
        "    similarity_model,\n",
        "    reward_calculator,\n",
        "    device\n",
        "):\n",
        "    \"\"\"Enhanced reward function with wider dynamic range (-12 to 10)\"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for i, (text, emotion, has_emotion, target_text, target_emotion) in enumerate(zip(\n",
        "        text_responses, emotion_responses, has_emotion_flags, target_texts, target_emotions\n",
        "    )):\n",
        "        quality_score = compute_text_quality_score_enhanced(text)\n",
        "        emotion_score = compute_emotion_score_enhanced(has_emotion, emotion, target_emotion)\n",
        "\n",
        "        user_input = user_inputs[i] if i < len(user_inputs) else \"\"\n",
        "        relevance_score = compute_contextual_relevance_enhanced(text, user_input, similarity_model, device)\n",
        "\n",
        "        # Sentiment and empathy scores (narrowed range)\n",
        "        if reward_calculator and text.strip():\n",
        "            sentiment_score = reward_calculator.compute_sentiment_reward(text) * 1.1 - 0.5  # [-0.5, 1.0]\n",
        "            empathy_score = reward_calculator.compute_empathy_reward(text) * 1.1 - 0.5      # [-0.5, 1.0]\n",
        "        else:\n",
        "            sentiment_score = 0.0\n",
        "            empathy_score = 0.0\n",
        "\n",
        "        # New penalty mechanisms\n",
        "        penalty = 0.0\n",
        "\n",
        "        # Emotional conflict penalty (user emotion vs response emotion)\n",
        "        if user_emotions and i < len(user_emotions):\n",
        "            user_emo = user_emotions[i].lower()\n",
        "            if (user_emo in [\"sadness\", \"depression\", \"anger\", \"disgust\", \"fear\"] and\n",
        "                emotion.lower() in [\"joy\"]):\n",
        "                penalty -= 1.5\n",
        "\n",
        "        # Balanced weighted calculation\n",
        "        total_reward = (\n",
        "            quality_score * 1.1 +        # Increased quality weight\n",
        "            emotion_score * 1.2 +\n",
        "            relevance_score * 1.1 +      # Increased relevance weight\n",
        "            sentiment_score * 0.7 +      # Reduced sentiment weight\n",
        "            empathy_score * 0.7 +        # Reduced empathy weight\n",
        "            penalty                      # New penalty term\n",
        "        )\n",
        "\n",
        "        bounded_reward = max(min(total_reward, 10.0), -10.0)\n",
        "        rewards.append(torch.tensor(bounded_reward, dtype=torch.float32))\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# ====================== Legacy Function Names (for compatibility) ======================\n",
        "def compute_text_quality_score_fixed(response):\n",
        "    \"\"\"Legacy wrapper for enhanced text quality scoring\"\"\"\n",
        "    return compute_text_quality_score_enhanced(response)\n",
        "\n",
        "def compute_emotion_score(has_emotion_token, generated_emotion, target_emotion):\n",
        "    \"\"\"Legacy wrapper for enhanced emotion scoring\"\"\"\n",
        "    return compute_emotion_score_enhanced(has_emotion_token, generated_emotion, target_emotion)\n",
        "\n",
        "def compute_contextual_relevance_score_progressive(generated_text, user_input, similarity_model, device):\n",
        "    \"\"\"Legacy wrapper for enhanced contextual relevance scoring\"\"\"\n",
        "    return compute_contextual_relevance_enhanced(generated_text, user_input, similarity_model, device)\n",
        "\n",
        "# ====================== Data Collator ======================\n",
        "def ppo_collator(batch):\n",
        "    \"\"\"PPO data collator\"\"\"\n",
        "    return {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask'], dtype=torch.long) for item in batch]),\n",
        "    }\n",
        "\n",
        "# ====================== Weights & Biases Logging ======================\n",
        "def log_sample_conversations(text_responses, user_inputs, batch_idx, epoch, max_samples=3):\n",
        "    \"\"\"Log sample conversation pairs to W&B\"\"\"\n",
        "    conversations = []\n",
        "    for i in range(min(len(text_responses), max_samples)):\n",
        "        conversations.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"batch\": batch_idx,\n",
        "            \"sample_id\": i,\n",
        "            \"user_input\": user_inputs[i],\n",
        "            \"therapist_response\": text_responses[i]\n",
        "        })\n",
        "\n",
        "    table = wandb.Table(\n",
        "        columns=[\"epoch\", \"batch\", \"sample_id\", \"user_input\", \"therapist_response\"],\n",
        "        data=[[conv[\"epoch\"], conv[\"batch\"], conv[\"sample_id\"],\n",
        "               conv[\"user_input\"], conv[\"therapist_response\"]] for conv in conversations]\n",
        "    )\n",
        "\n",
        "    wandb.log({\"sample_conversations\": table})\n",
        "\n",
        "def log_score_distributions(scores_dict, step):\n",
        "    \"\"\"Log score distributions as histograms\"\"\"\n",
        "    for name, scores in scores_dict.items():\n",
        "        if scores and len(scores) > 0:\n",
        "            wandb.log({\n",
        "                f\"distributions/{name}\": wandb.Histogram(scores),\n",
        "                f\"distributions/{name}_mean\": np.mean(scores),\n",
        "                f\"distributions/{name}_std\": np.std(scores)\n",
        "            }, step=step)\n",
        "\n",
        "# ====================== Main Training Function ======================\n",
        "def run_ppo_training_with_wandb(\n",
        "    train_data_path,\n",
        "    val_data_path,\n",
        "    sft_model_path,\n",
        "    tokenizer_path,\n",
        "    num_epochs=3,\n",
        "    batch_size=8,\n",
        "    save_dir=\"ppo_model_with_wandb\",\n",
        "    project_name=\"therapy-ppo-training\",\n",
        "    run_name=None\n",
        "):\n",
        "    \"\"\"Run PPO training with comprehensive W&B logging\"\"\"\n",
        "\n",
        "    print(\"Starting PPO training with Weights & Biases integration...\")\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if run_name is None:\n",
        "        run_name = f\"8m18therapy_ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        name=run_name,\n",
        "        config={\n",
        "            \"architecture\": \"GPT2\",\n",
        "            \"dataset\": \"therapy_conversations\",\n",
        "            \"num_epochs\": num_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"max_length\": 128,\n",
        "            \"max_new_tokens\": 128,\n",
        "            \"relevance_function\": \"enhanced_semantic_scoring\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"W&B run: {wandb.run.name}\")\n",
        "    print(f\"W&B url: {wandb.run.url}\")\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "    # Load base model\n",
        "    print(\"Loading base model...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Load SFT checkpoint\n",
        "    print(f\"Loading SFT checkpoint from {sft_model_path}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(sft_model_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            state_dict = checkpoint['model_state_dict']\n",
        "        elif 'state_dict' in checkpoint:\n",
        "            state_dict = checkpoint['state_dict']\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "\n",
        "        cleaned_state_dict = {}\n",
        "        for key, value in state_dict.items():\n",
        "            if key.startswith('model.'):\n",
        "                cleaned_state_dict[key[6:]] = value\n",
        "            else:\n",
        "                cleaned_state_dict[key] = value\n",
        "\n",
        "        model.load_state_dict(cleaned_state_dict, strict=False)\n",
        "        model.to(device)\n",
        "        print(\"SFT checkpoint loaded successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading SFT checkpoint: {e}\")\n",
        "        print(\"Continuing with base model...\")\n",
        "\n",
        "    # Initialize similarity model (REQUIRED)\n",
        "    print(\"Loading similarity model...\")\n",
        "    try:\n",
        "        similarity_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "        print(\"Similarity model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Failed to load similarity model: {e}\")\n",
        "        print(\"Similarity model is required for relevance scoring. Training cannot continue.\")\n",
        "        raise RuntimeError(\"Similarity model is required but failed to load\")\n",
        "\n",
        "    # Initialize enhanced reward calculator with sentiment and empathy models\n",
        "    print(\"Loading sentiment and empathy models...\")\n",
        "    try:\n",
        "        reward_calculator = TherapyRewardCalculator(device)\n",
        "        print(\"Sentiment and empathy models loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sentiment/empathy models: {e}\")\n",
        "        reward_calculator = None\n",
        "\n",
        "    # Create PPO configuration\n",
        "    print(\"Creating PPO configuration...\")\n",
        "\n",
        "    ppo_config = PPOConfig(\n",
        "        model_name='gpt2',\n",
        "        whiten_rewards=True,\n",
        "        learning_rate=1e-6,\n",
        "        batch_size=batch_size,\n",
        "        mini_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=1,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    # Log PPO hyperparameters to W&B\n",
        "    wandb.config.update({\n",
        "        \"ppo_learning_rate\": ppo_config.learning_rate,\n",
        "        \"ppo_batch_size\": ppo_config.batch_size,\n",
        "        \"ppo_whiten_rewards\": ppo_config.whiten_rewards,\n",
        "        \"generation_top_p\": 1.0,\n",
        "        \"generation_top_k\": 0.0,\n",
        "        \"enhanced_rewards\": \"comprehensive_enhanced_scoring\",\n",
        "        \"quality_function\": \"enhanced_repetition_detection\",\n",
        "        \"relevance_function\": \"enhanced_semantic_similarity\",\n",
        "        \"data_alignment\": \"custom_trainer_no_shuffle\"\n",
        "    })\n",
        "\n",
        "    # Create PPO models\n",
        "    print(\"Creating PPO models...\")\n",
        "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "        model,\n",
        "        torch_dtype=torch.float32\n",
        "    )\n",
        "    ppo_model.to(device)\n",
        "\n",
        "    ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "        model,\n",
        "        torch_dtype=torch.float32\n",
        "    )\n",
        "    ref_model.to(device)\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_therapy_dataset = TherapyDatasetV4(train_data_path, tokenizer_path)\n",
        "    val_therapy_dataset = TherapyDatasetV4(val_data_path, tokenizer_path)\n",
        "\n",
        "    full_train_dataset = train_therapy_dataset.to_hf_dataset()\n",
        "    full_val_dataset = val_therapy_dataset.to_hf_dataset()\n",
        "\n",
        "    # Extract metadata\n",
        "    train_target_texts = full_train_dataset['target_text']\n",
        "    train_target_emotions = full_train_dataset['target_emotion']\n",
        "    train_queries = full_train_dataset['query']\n",
        "    train_user_inputs = full_train_dataset['user_input']\n",
        "    train_user_emotions = full_train_dataset['user_emotion']\n",
        "\n",
        "    val_target_texts = full_val_dataset['target_text']\n",
        "    val_target_emotions = full_val_dataset['target_emotion']\n",
        "    val_user_inputs = full_val_dataset['user_input']\n",
        "    val_user_emotions = full_val_dataset['user_emotion']\n",
        "\n",
        "    # Create PPO datasets\n",
        "    train_dataset = full_train_dataset.remove_columns([\n",
        "        'target_text', 'target_emotion', 'query', 'user_input', 'user_emotion'\n",
        "    ])\n",
        "    val_dataset = full_val_dataset.remove_columns([\n",
        "        'target_text', 'target_emotion', 'query', 'user_input', 'user_emotion'\n",
        "    ])\n",
        "\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Log dataset info to W&B\n",
        "    wandb.config.update({\n",
        "        \"train_dataset_size\": len(train_dataset),\n",
        "        \"val_dataset_size\": len(val_dataset),\n",
        "        \"total_parameters\": sum(p.numel() for p in ppo_model.parameters()),\n",
        "        \"trainable_parameters\": sum(p.numel() for p in ppo_model.parameters() if p.requires_grad)\n",
        "    })\n",
        "\n",
        "    # Initialize CUSTOM PPO trainer (prevents data shuffling)\n",
        "    print(\"Initializing Custom PPO trainer...\")\n",
        "    ppo_trainer = CustomPPOTrainer(\n",
        "        config=ppo_config,\n",
        "        model=ppo_model,\n",
        "        ref_model=ref_model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=train_dataset,\n",
        "        data_collator=ppo_collator\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting PPO training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Epoch metrics\n",
        "        epoch_rewards = []\n",
        "        epoch_quality_scores = []\n",
        "        epoch_emotion_scores = []\n",
        "        epoch_relevance_scores = []\n",
        "        epoch_sentiment_scores = []\n",
        "        epoch_empathy_scores = []\n",
        "        epoch_emotion_success = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=\"Training batches\")):\n",
        "            try:\n",
        "                query_tensors = [item.to(device) for item in batch[\"input_ids\"]]\n",
        "\n",
        "                # Get current batch data with perfect alignment (no shuffling)\n",
        "                start_idx = batch_idx * batch_size\n",
        "                end_idx = min(start_idx + batch_size, len(train_target_texts))\n",
        "                current_batch_size = end_idx - start_idx\n",
        "\n",
        "                # Skip if we're beyond the data bounds\n",
        "                if start_idx >= len(train_target_texts):\n",
        "                    continue\n",
        "\n",
        "                # Extract metadata for current batch\n",
        "                batch_target_texts = train_target_texts[start_idx:end_idx]\n",
        "                batch_target_emotions = train_target_emotions[start_idx:end_idx]\n",
        "                batch_user_inputs = train_user_inputs[start_idx:end_idx]\n",
        "                batch_user_emotions = train_user_emotions[start_idx:end_idx]\n",
        "\n",
        "                # Ensure tensor batch size matches metadata batch size\n",
        "                query_tensors = query_tensors[:current_batch_size]\n",
        "\n",
        "                # Generate responses\n",
        "                response_tensors = []\n",
        "                text_responses = []\n",
        "                emotion_responses = []\n",
        "                has_emotion_flags = []\n",
        "\n",
        "                for query_tensor in query_tensors:\n",
        "                    response_tokens, therapist_text, therapist_emotion, has_emotion = generate_therapy_response(\n",
        "                        ppo_model.pretrained_model,\n",
        "                        tokenizer,\n",
        "                        query_tensor,\n",
        "                        device\n",
        "                    )\n",
        "                    response_tensors.append(response_tokens)\n",
        "                    text_responses.append(therapist_text)\n",
        "                    emotion_responses.append(therapist_emotion)\n",
        "                    has_emotion_flags.append(has_emotion)\n",
        "\n",
        "                # Compute rewards with ENHANCED scoring\n",
        "                rewards = compute_comprehensive_rewards_enhanced(\n",
        "                    text_responses,\n",
        "                    emotion_responses,\n",
        "                    has_emotion_flags,\n",
        "                    batch_target_texts,\n",
        "                    batch_target_emotions,\n",
        "                    batch_user_inputs,\n",
        "                    batch_user_emotions,\n",
        "                    similarity_model,\n",
        "                    reward_calculator,\n",
        "                    device\n",
        "                )\n",
        "\n",
        "                # Compute individual component scores for detailed logging\n",
        "                batch_quality_scores = [compute_text_quality_score_enhanced(text) for text in text_responses]\n",
        "                batch_emotion_scores = [\n",
        "                    compute_emotion_score_enhanced(has_emotion, emotion, target_emotion)\n",
        "                    for has_emotion, emotion, target_emotion in zip(has_emotion_flags, emotion_responses, batch_target_emotions)\n",
        "                ]\n",
        "                batch_relevance_scores = [\n",
        "                    compute_contextual_relevance_enhanced(text, user_input, similarity_model, device)\n",
        "                    for text, user_input in zip(text_responses, batch_user_inputs)\n",
        "                ]\n",
        "\n",
        "                # Track sentiment and empathy scores\n",
        "                batch_sentiment_scores = [\n",
        "                    reward_calculator.compute_sentiment_reward(text) if reward_calculator and text.strip() else 0.0\n",
        "                    for text in text_responses\n",
        "                ]\n",
        "                batch_empathy_scores = [\n",
        "                    reward_calculator.compute_empathy_reward(text) if reward_calculator and text.strip() else 0.0\n",
        "                    for text in text_responses\n",
        "                ]\n",
        "\n",
        "                # Track epoch metrics\n",
        "                epoch_rewards.extend([r.item() for r in rewards])\n",
        "                epoch_quality_scores.extend(batch_quality_scores)\n",
        "                epoch_emotion_scores.extend(batch_emotion_scores)\n",
        "                epoch_relevance_scores.extend(batch_relevance_scores)\n",
        "                epoch_sentiment_scores.extend(batch_sentiment_scores)\n",
        "                epoch_empathy_scores.extend(batch_empathy_scores)\n",
        "                epoch_emotion_success.extend(has_emotion_flags)\n",
        "\n",
        "                # PPO training step\n",
        "                stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "                # W&B logging for this batch\n",
        "                batch_metrics = {\n",
        "                    \"train/batch_reward\": np.mean([r.item() for r in rewards]),\n",
        "                    \"train/batch_quality\": np.mean(batch_quality_scores),\n",
        "                    \"train/batch_emotion\": np.mean(batch_emotion_scores),\n",
        "                    \"train/batch_relevance\": np.mean(batch_relevance_scores),\n",
        "                    \"train/batch_sentiment\": np.mean(batch_sentiment_scores),\n",
        "                    \"train/batch_empathy\": np.mean(batch_empathy_scores),\n",
        "                    \"train/batch_emotion_success_rate\": np.mean(has_emotion_flags),\n",
        "                    \"train/epoch\": epoch,\n",
        "                    \"train/batch\": batch_idx,\n",
        "                }\n",
        "\n",
        "                # Log PPO stats if available\n",
        "                if stats:\n",
        "                    for key, value in stats.items():\n",
        "                        if isinstance(value, (int, float, torch.Tensor)):\n",
        "                            if isinstance(value, torch.Tensor):\n",
        "                                value = value.item()\n",
        "                            batch_metrics[f\"ppo/{key}\"] = value\n",
        "\n",
        "                wandb.log(batch_metrics, step=global_step)\n",
        "\n",
        "                # Log sample responses periodically\n",
        "                if batch_idx % 10 == 0:\n",
        "                    log_sample_conversations(text_responses, batch_user_inputs, batch_idx, epoch)\n",
        "\n",
        "                # Progress logging\n",
        "                if batch_idx % 5 == 0:\n",
        "                    avg_reward = np.mean([r.item() for r in rewards])\n",
        "                    avg_quality = np.mean(batch_quality_scores)\n",
        "                    avg_emotion = np.mean(batch_emotion_scores)\n",
        "                    avg_relevance = np.mean(batch_relevance_scores)\n",
        "                    avg_sentiment = np.mean(batch_sentiment_scores)\n",
        "                    avg_empathy = np.mean(batch_empathy_scores)\n",
        "                    emotion_success_rate = np.mean(has_emotion_flags)\n",
        "\n",
        "                    print(f\"Batch {batch_idx}: Reward={avg_reward:.3f}, Quality={avg_quality:.3f}, \" +\n",
        "                          f\"Emotion={avg_emotion:.3f}, Relevance={avg_relevance:.3f}, \" +\n",
        "                          f\"Sentiment={avg_sentiment:.3f}, Empathy={avg_empathy:.3f}, Success={emotion_success_rate:.2%}\")\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Epoch summary\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        avg_epoch_reward = np.mean(epoch_rewards) if epoch_rewards else 0.0\n",
        "        avg_epoch_quality = np.mean(epoch_quality_scores) if epoch_quality_scores else 0.0\n",
        "        avg_epoch_emotion = np.mean(epoch_emotion_scores) if epoch_emotion_scores else 0.0\n",
        "        avg_epoch_relevance = np.mean(epoch_relevance_scores) if epoch_relevance_scores else 0.0\n",
        "        avg_epoch_sentiment = np.mean(epoch_sentiment_scores) if epoch_sentiment_scores else 0.0\n",
        "        avg_epoch_empathy = np.mean(epoch_empathy_scores) if epoch_empathy_scores else 0.0\n",
        "        emotion_success_rate = np.mean(epoch_emotion_success) if epoch_emotion_success else 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "        print(f\"Average total reward: {avg_epoch_reward:.4f}\")\n",
        "        print(f\"Average quality score: {avg_epoch_quality:.4f}\")\n",
        "        print(f\"Average emotion score: {avg_epoch_emotion:.4f}\")\n",
        "        print(f\"Average relevance score: {avg_epoch_relevance:.4f}\")\n",
        "        print(f\"Average sentiment score: {avg_epoch_sentiment:.4f}\")\n",
        "        print(f\"Average empathy score: {avg_epoch_empathy:.4f}\")\n",
        "        print(f\"Emotion generation success: {emotion_success_rate:.2%}\")\n",
        "        print(f\"Epoch time: {epoch_time:.1f}s\")\n",
        "\n",
        "        # Log epoch metrics and distributions\n",
        "        epoch_metrics = {\n",
        "            \"epoch/reward\": avg_epoch_reward,\n",
        "            \"epoch/quality\": avg_epoch_quality,\n",
        "            \"epoch/emotion\": avg_epoch_emotion,\n",
        "            \"epoch/relevance\": avg_epoch_relevance,\n",
        "            \"epoch/sentiment\": avg_epoch_sentiment,\n",
        "            \"epoch/empathy\": avg_epoch_empathy,\n",
        "            \"epoch/emotion_success_rate\": emotion_success_rate,\n",
        "            \"epoch/time_seconds\": epoch_time,\n",
        "            \"epoch/number\": epoch + 1,\n",
        "        }\n",
        "\n",
        "        wandb.log(epoch_metrics, step=global_step)\n",
        "\n",
        "        # Log score distributions\n",
        "        log_score_distributions({\n",
        "            \"reward\": epoch_rewards,\n",
        "            \"quality\": epoch_quality_scores,\n",
        "            \"emotion\": epoch_emotion_scores,\n",
        "            \"relevance\": epoch_relevance_scores,\n",
        "            \"sentiment\": epoch_sentiment_scores,\n",
        "            \"empathy\": epoch_empathy_scores\n",
        "        }, global_step)\n",
        "\n",
        "        # Save checkpoint with tokenizer\n",
        "        epoch_save_path = os.path.join(save_dir, f\"epoch_{epoch+1}\")\n",
        "        ppo_trainer.save_pretrained(epoch_save_path)\n",
        "        tokenizer.save_pretrained(epoch_save_path)  # Save tokenizer too\n",
        "        print(f\"Saved checkpoint to {epoch_save_path}\")\n",
        "\n",
        "        # Log model checkpoint as W&B artifact\n",
        "        artifact = wandb.Artifact(f\"model_epoch_{epoch+1}\", type=\"model\")\n",
        "        artifact.add_dir(epoch_save_path)\n",
        "        wandb.log_artifact(artifact)\n",
        "\n",
        "        # Validation\n",
        "        print(\"Running validation...\")\n",
        "        val_start_time = time.time()\n",
        "\n",
        "        val_rewards = []\n",
        "        val_quality_scores = []\n",
        "        val_emotion_scores = []\n",
        "        val_relevance_scores = []\n",
        "        val_sentiment_scores = []\n",
        "        val_empathy_scores = []\n",
        "        val_emotion_success = []\n",
        "\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=ppo_collator, shuffle=False)\n",
        "\n",
        "        for val_batch_idx, val_batch in enumerate(tqdm(val_dataloader, desc=\"Validation\", leave=False)):\n",
        "            try:\n",
        "                query_tensors = [item.to(device) for item in val_batch[\"input_ids\"]]\n",
        "\n",
        "                # Use safer indexing for validation too\n",
        "                actual_batch_size = len(query_tensors)\n",
        "                start_idx = val_batch_idx * batch_size\n",
        "                end_idx = min(start_idx + batch_size, len(val_target_texts))\n",
        "                current_batch_size = min(actual_batch_size, end_idx - start_idx)\n",
        "\n",
        "                # Skip if we're beyond the data bounds\n",
        "                if start_idx >= len(val_target_texts):\n",
        "                    continue\n",
        "\n",
        "                # Use only the valid portion\n",
        "                query_tensors = query_tensors[:current_batch_size]\n",
        "                batch_target_texts = val_target_texts[start_idx:start_idx + current_batch_size]\n",
        "                batch_target_emotions = val_target_emotions[start_idx:start_idx + current_batch_size]\n",
        "                batch_user_inputs = val_user_inputs[start_idx:start_idx + current_batch_size]\n",
        "                batch_user_emotions = val_user_emotions[start_idx:start_idx + current_batch_size]\n",
        "\n",
        "                text_responses = []\n",
        "                emotion_responses = []\n",
        "                has_emotion_flags = []\n",
        "\n",
        "                for query_tensor in query_tensors:\n",
        "                    _, therapist_text, therapist_emotion, has_emotion = generate_therapy_response(\n",
        "                        ppo_model.pretrained_model,\n",
        "                        tokenizer,\n",
        "                        query_tensor,\n",
        "                        device\n",
        "                    )\n",
        "                    text_responses.append(therapist_text)\n",
        "                    emotion_responses.append(therapist_emotion)\n",
        "                    has_emotion_flags.append(has_emotion)\n",
        "\n",
        "                rewards = compute_comprehensive_rewards_enhanced(\n",
        "                    text_responses,\n",
        "                    emotion_responses,\n",
        "                    has_emotion_flags,\n",
        "                    batch_target_texts,\n",
        "                    batch_target_emotions,\n",
        "                    batch_user_inputs,\n",
        "                    batch_user_emotions,\n",
        "                    similarity_model,\n",
        "                    reward_calculator,\n",
        "                    device\n",
        "                )\n",
        "\n",
        "                # Compute component scores\n",
        "                batch_quality_scores = [compute_text_quality_score_enhanced(text) for text in text_responses]\n",
        "                batch_emotion_scores = [\n",
        "                    compute_emotion_score_enhanced(has_emotion, emotion, target_emotion)\n",
        "                    for has_emotion, emotion, target_emotion in zip(has_emotion_flags, emotion_responses, batch_target_emotions)\n",
        "                ]\n",
        "                batch_relevance_scores = [\n",
        "                    compute_contextual_relevance_enhanced(text, user_input, similarity_model, device)\n",
        "                    for text, user_input in zip(text_responses, batch_user_inputs)\n",
        "                ]\n",
        "                batch_sentiment_scores = [\n",
        "                    reward_calculator.compute_sentiment_reward(text) if reward_calculator and text.strip() else 0.0\n",
        "                    for text in text_responses\n",
        "                ]\n",
        "                batch_empathy_scores = [\n",
        "                    reward_calculator.compute_empathy_reward(text) if reward_calculator and text.strip() else 0.0\n",
        "                    for text in text_responses\n",
        "                ]\n",
        "\n",
        "                val_rewards.extend([r.item() for r in rewards])\n",
        "                val_quality_scores.extend(batch_quality_scores)\n",
        "                val_emotion_scores.extend(batch_emotion_scores)\n",
        "                val_relevance_scores.extend(batch_relevance_scores)\n",
        "                val_sentiment_scores.extend(batch_sentiment_scores)\n",
        "                val_empathy_scores.extend(batch_empathy_scores)\n",
        "                val_emotion_success.extend(has_emotion_flags)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in validation batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        val_time = time.time() - val_start_time\n",
        "\n",
        "        val_avg_reward = np.mean(val_rewards) if val_rewards else 0.0\n",
        "        val_avg_quality = np.mean(val_quality_scores) if val_quality_scores else 0.0\n",
        "        val_avg_emotion = np.mean(val_emotion_scores) if val_emotion_scores else 0.0\n",
        "        val_avg_relevance = np.mean(val_relevance_scores) if val_relevance_scores else 0.0\n",
        "        val_avg_sentiment = np.mean(val_sentiment_scores) if val_sentiment_scores else 0.0\n",
        "        val_avg_empathy = np.mean(val_empathy_scores) if val_empathy_scores else 0.0\n",
        "        val_emotion_success_rate = np.mean(val_emotion_success) if val_emotion_success else 0.0\n",
        "\n",
        "        print(f\"Validation Results:\")\n",
        "        print(f\"Average reward: {val_avg_reward:.4f}\")\n",
        "        print(f\"Average quality: {val_avg_quality:.4f}\")\n",
        "        print(f\"Average emotion: {val_avg_emotion:.4f}\")\n",
        "        print(f\"Average relevance: {val_avg_relevance:.4f}\")\n",
        "        print(f\"Average sentiment: {val_avg_sentiment:.4f}\")\n",
        "        print(f\"Average empathy: {val_avg_empathy:.4f}\")\n",
        "        print(f\"Emotion generation success: {val_emotion_success_rate:.2%}\")\n",
        "        print(f\"Validation time: {val_time:.1f}s\")\n",
        "\n",
        "        # Log validation metrics\n",
        "        val_metrics = {\n",
        "            \"val/reward\": val_avg_reward,\n",
        "            \"val/quality\": val_avg_quality,\n",
        "            \"val/emotion\": val_avg_emotion,\n",
        "            \"val/relevance\": val_avg_relevance,\n",
        "            \"val/sentiment\": val_avg_sentiment,\n",
        "            \"val/empathy\": val_avg_empathy,\n",
        "            \"val/emotion_success_rate\": val_emotion_success_rate,\n",
        "            \"val/time_seconds\": val_time,\n",
        "        }\n",
        "\n",
        "        wandb.log(val_metrics, step=global_step)\n",
        "\n",
        "        # Log validation score distributions\n",
        "        log_score_distributions({\n",
        "            \"val_reward\": val_rewards,\n",
        "            \"val_quality\": val_quality_scores,\n",
        "            \"val_emotion\": val_emotion_scores,\n",
        "            \"val_relevance\": val_relevance_scores,\n",
        "            \"val_sentiment\": val_sentiment_scores,\n",
        "            \"val_empathy\": val_empathy_scores\n",
        "        }, global_step)\n",
        "\n",
        "    # Save final model with tokenizer\n",
        "    final_save_path = os.path.join(save_dir, \"final_model\")\n",
        "    ppo_trainer.save_pretrained(final_save_path)\n",
        "    tokenizer.save_pretrained(final_save_path)  # Save tokenizer too\n",
        "\n",
        "    # Log final model as artifact\n",
        "    final_artifact = wandb.Artifact(\"final_model\", type=\"model\")\n",
        "    final_artifact.add_dir(final_save_path)\n",
        "    wandb.log_artifact(final_artifact)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed! Total time: {total_time:.1f}s\")\n",
        "    print(f\"Final model saved to {final_save_path}\")\n",
        "    print(f\"W&B run url: {wandb.run.url}\")\n",
        "\n",
        "    # Log final summary\n",
        "    wandb.log({\n",
        "        \"training/total_time_seconds\": total_time,\n",
        "        \"training/total_steps\": global_step,\n",
        "        \"training/epochs_completed\": num_epochs\n",
        "    })\n",
        "\n",
        "    # Finish W&B run\n",
        "    wandb.finish()\n",
        "\n",
        "    return ppo_trainer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dataset = \"/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/train.json\"\n",
        "    val_dataset = \"/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/val.json\"\n",
        "    sft_model = \"/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/therapy_model_4thFIXED_epoch_7_loss_2.2373.ckpt\"\n",
        "    tokenizer_path = \"/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/train_processed_4thFIXED_tokenizer\"\n",
        "\n",
        "    try:\n",
        "        ppo_model = run_ppo_training_with_wandb(\n",
        "            train_data_path=train_dataset,\n",
        "            val_data_path=val_dataset,\n",
        "            sft_model_path=sft_model,\n",
        "            tokenizer_path=tokenizer_path,\n",
        "            num_epochs=8,\n",
        "            batch_size=16,\n",
        "            save_dir=\"/content/drive/MyDrive/RL-SFT-GPT2 MentalHealth/RL<token>8 6/8m19enhanced_ppo_model\",\n",
        "            project_name=\"8m19therapy-chatbot-ppo-enhanced\",\n",
        "            run_name=\"8m19therapy_ppo_enhanced_rewards\"\n",
        "        )\n",
        "\n",
        "        print(\"PPO training with enhanced reward components completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
